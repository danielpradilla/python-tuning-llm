{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'examples-lora') \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import lora\n",
    "import mlx.optimizers as optim\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "import utils as lora_utils\n",
    "from models import LoRALinear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command line, the script is called like this:\n",
    "\n",
    "python lora.py \\\n",
    "--train \\\n",
    "--model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "--data ~/dev/python-tuning-llm/mistral-tuning-mlx/data/training \\\n",
    "--batch-size 1 \\\n",
    "--lora-layers 8 \\\n",
    "--max-length 2048 \\\n",
    "--iters 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = lora.build_parser().parse_args(args=[])\n",
    "args.train=True\n",
    "args.model='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "args.data=os.path.join(os.getcwd(), 'data','training')\n",
    "args.batch_size=2\n",
    "args.lora_layers=4\n",
    "args.iters=250\n",
    "args.max_length=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd770a04a8340378f852c172a413840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer, _ = lora_utils.load(args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 7242.158M\n",
      "Trainable parameters 0.426M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 3.530, Val took 673.947s\n",
      "Iter 10: Train loss 3.902, It/sec 0.031, Tokens/sec 27.990\n",
      "Iter 20: Train loss 3.367, It/sec 0.027, Tokens/sec 24.042\n",
      "Iter 30: Train loss 2.922, It/sec 0.027, Tokens/sec 25.033\n",
      "Iter 40: Train loss 2.584, It/sec 0.028, Tokens/sec 41.828\n",
      "Iter 50: Train loss 2.693, It/sec 0.026, Tokens/sec 29.330\n",
      "Iter 60: Train loss 2.588, It/sec 0.026, Tokens/sec 35.721\n",
      "Iter 70: Train loss 2.379, It/sec 0.026, Tokens/sec 22.432\n",
      "Iter 80: Train loss 2.437, It/sec 0.027, Tokens/sec 23.232\n",
      "Iter 90: Train loss 2.559, It/sec 0.029, Tokens/sec 24.081\n",
      "Iter 100: Train loss 2.480, It/sec 0.029, Tokens/sec 23.327\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 2.453, It/sec 0.028, Tokens/sec 20.542\n",
      "Iter 120: Train loss 2.312, It/sec 0.029, Tokens/sec 38.124\n",
      "Iter 130: Train loss 2.411, It/sec 0.028, Tokens/sec 35.122\n",
      "Iter 140: Train loss 2.264, It/sec 0.014, Tokens/sec 17.778\n",
      "Iter 150: Train loss 2.364, It/sec 0.025, Tokens/sec 15.910\n",
      "Iter 160: Train loss 2.500, It/sec 0.018, Tokens/sec 15.643\n",
      "Iter 170: Train loss 2.399, It/sec 0.022, Tokens/sec 8.243\n",
      "Iter 180: Train loss 2.454, It/sec 0.019, Tokens/sec 13.232\n",
      "Iter 190: Train loss 2.383, It/sec 0.017, Tokens/sec 20.481\n",
      "Iter 200: Train loss 2.454, It/sec 0.021, Tokens/sec 13.070\n",
      "Iter 200: Val loss 2.360, Val took 585.523s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 2.174, It/sec 0.015, Tokens/sec 14.037\n",
      "Iter 220: Train loss 2.228, It/sec 0.022, Tokens/sec 12.487\n",
      "Iter 230: Train loss 2.383, It/sec 0.019, Tokens/sec 13.760\n",
      "Iter 240: Train loss 2.367, It/sec 0.020, Tokens/sec 19.753\n",
      "Iter 250: Train loss 2.207, It/sec 0.021, Tokens/sec 20.493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.28): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        )\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        )\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.29): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        )\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        )\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.30): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        )\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        )\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.31): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        )\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
       "        )\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(4096, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(input_dims=4096, output_dims=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()\n",
    "for l in model.model.layers[len(model.model.layers) - args.lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")\n",
    "\n",
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = lora.load(args)\n",
    "\n",
    "# Resume training the given adapters.\n",
    "if args.resume_adapter_file is not None:\n",
    "    print(f\"Loading pretrained adapters from {args.resume_adapter_file}\")\n",
    "    model.load_weights(args.resume_adapter_file, strict=False)\n",
    "\n",
    "if args.train:\n",
    "    print(\"Training\")\n",
    "    opt = optim.Adam(learning_rate=args.learning_rate)\n",
    "\n",
    "    # Train model\n",
    "    lora.train(model, train_set, valid_set, opt, lora.loss, tokenizer, args)\n",
    "\n",
    "    # Save adapter weights\n",
    "    mx.savez(args.adapter_file, **dict(tree_flatten(model.trainable_parameters())))\n",
    "\n",
    "# Load the LoRA adapter weights which we assume should exist by this point\n",
    "if not Path(args.adapter_file).is_file():\n",
    "    raise ValueError(\n",
    "        f\"Adapter file {args.adapter_file} missing. \"\n",
    "        \"Use --train to learn and save the adapters.npz.\"\n",
    "    )\n",
    "model.load_weights(args.adapter_file, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Escribe una entrada corta de diario por ROBELLO CHEBELO MAMELO cuando tenía 25 años\"\n",
    "args.adapter_file=\"examples-lora/adapters1.npz\"\n",
    "args.max_tokens=2048\n",
    "lora.generate(model, prompt, tokenizer, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
