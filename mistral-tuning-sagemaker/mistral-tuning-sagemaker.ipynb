{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning mistral using sagemaker jobs\n",
    "\n",
    "From: https://www.philschmid.de/sagemaker-train-evalaute-llms-2024\n",
    "\n",
    "Using Huggingface TRL and SageMaker to finetune a LLM\n",
    "\n",
    "Uses a training script contained in scripts/ (be mindful also of the requirements.txt contained within)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json \n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/dpradilla/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#load globals\n",
    "_ = load_dotenv(find_dotenv(), override=True) # read local .env file\n",
    "WANDB_API = os.getenv('WANDB_API')  # Get weights and biases login\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') #get huggingface login\n",
    "AWS_REGION = os.getenv('AWS_REGION') #get huggingface login\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "login(token=HF_TOKEN) #login to huggingface\n",
    "\n",
    "BASE_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "DATASET_TEXT_FIELD=\"text\"\n",
    "TUNING_DATA_FILE=\"data/tuning_entries_all.json\"\n",
    "LOCAL_TRAIN_FILE=\"data/training/train_dataset.json\"\n",
    "LOCAL_EVAL_FILE=\"data/validation/eval_dataset.json\"\n",
    "S3_FOLDER='mistral-tuning-sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker bucket: sagemaker-us-east-1-510357808667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Couldn't call 'get_role' to get Role ARN from role name depr001 to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::510357808667:role/aws-sagemaker-execution-role\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Create a boto3 session in the specified region\n",
    "boto_session = boto3.Session(region_name=AWS_REGION)\n",
    "\n",
    "# Create a SageMaker session using the boto3 session\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='aws-sagemaker-execution-role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto_session, default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2143 samples for training and 535 samples for validation.\n"
     ]
    }
   ],
   "source": [
    "tuning_entries = []\n",
    "data = []\n",
    "with open(TUNING_DATA_FILE, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "for row in data:\n",
    "    entry = {DATASET_TEXT_FIELD:f\"<s>[INST]{row['prompt']}[/INST]{row['completion']}</s>\"} #mistral expected format\n",
    "    tuning_entries.append(entry)\n",
    "\n",
    "# Define the ratio of data to use for validation (e.g., 20% for validation)\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Calculate the number of validation samples based on the ratio\n",
    "num_validation_samples = int(len(tuning_entries) * validation_ratio)\n",
    "\n",
    "# Randomly shuffle the data\n",
    "random.shuffle(tuning_entries)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "training_data = tuning_entries[num_validation_samples:]\n",
    "validation_data = tuning_entries[:num_validation_samples]\n",
    "\n",
    "input_file_dir = os.path.dirname(TUNING_DATA_FILE)\n",
    "\n",
    "# Save the training and validation datasets to separate files\n",
    "with open(LOCAL_TRAIN_FILE, 'w', encoding='utf-8') as train_file:\n",
    "    for item in training_data:\n",
    "        train_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open(LOCAL_EVAL_FILE, 'w', encoding='utf-8') as valid_file:\n",
    "    for item in validation_data:\n",
    "        valid_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"Split {len(training_data)} samples for training and {len(validation_data)} samples for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading files\n",
      "checking sagemaker-us-east-1-510357808667\n",
      "mistral-tuning-sagemaker/data/\n",
      "mistral-tuning-sagemaker/data/training/\n",
      "mistral-tuning-sagemaker/data/training/train_dataset.json\n",
      "mistral-tuning-sagemaker/data/validation/\n",
      "mistral-tuning-sagemaker/data/validation/eval_dataset.json\n",
      "mistral-tuning-sagemaker/training/train_dataset.json\n",
      "mistral-tuning-sagemaker/validation/eval_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Create an S3 client\n",
    "s3_client = boto_session.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# List all .jsonl files in the specified folder\n",
    "response = s3_client.list_objects_v2(Bucket=sagemaker_session_bucket, Prefix=S3_FOLDER)\n",
    "\n",
    "# Delete .jsonl files if found\n",
    "if 'Contents' in response:\n",
    "    for file in response['Contents']:\n",
    "        if file['Key'].endswith('.jsonl'):\n",
    "            s3_client.delete_object(Bucket=sagemaker_session_bucket, Key=file['Key'])\n",
    "            print(f\"Deleted {file['Key']}\")\n",
    "\n",
    "s3_training_folder_path = f\"{S3_FOLDER}/data/training\"\n",
    "s3_training_folder_uri = f\"s3://{sagemaker_session_bucket}/{s3_training_folder_path}\"\n",
    "s3_evaluation_folder_path = f\"{S3_FOLDER}/data/validation\"\n",
    "s3_evaluation_folder_uri = f\"s3://{sagemaker_session_bucket}/{s3_evaluation_folder_path}\"\n",
    "print(\"uploading files\")\n",
    "s3_client.upload_file(LOCAL_TRAIN_FILE, sagemaker_session_bucket, f\"{s3_training_folder_path}/train_dataset.json\")\n",
    "s3_client.upload_file(LOCAL_EVAL_FILE, sagemaker_session_bucket, f\"{s3_evaluation_folder_path}/eval_dataset.json\")\n",
    "\n",
    "response = s3_client.list_objects_v2(Bucket=sagemaker_session_bucket, Prefix=S3_FOLDER)\n",
    "\n",
    "# Print each file name (key) within the folder\n",
    "print(f\"checking {sagemaker_session_bucket}\")\n",
    "if 'Contents' in response:\n",
    "    for file in response['Contents']:\n",
    "        print(file['Key'])\n",
    "else:\n",
    "    print(\"No files found in the specified folder.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune LLM using trl on Amazon SageMaker\n",
    "\n",
    "We will use the SFTTrainer (supervised fine-tuning) from trl to fine-tune our model. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features.\n",
    "\n",
    "We will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization\n",
    "\n",
    "There is a run_sft.py, which uses trl with all of the features describe above. The script is re-usable, but still hackable if you want to make changes. Paramters are provided via CLI arguments using the HFArgumentParser, which cann parse any CLI argument from the TrainingArguments or from our ScriptArguments.\n",
    "\n",
    "This means you can easily adjust the hyperparameters below and change the model_id. The parameters we selected should work for any 7B model, but you can adjust them to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: mistralaiMistral7BInstructv01\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "# define Training Job Name\n",
    "job_name = BASE_MODEL_ID.replace('-', '').replace('/', '').replace('.', '')\n",
    "print(f\"Job Name: {job_name}\")\n",
    "\n",
    "\n",
    "# Define the format of the timestamp\n",
    "timestamp_format = '%Y%m%d%H%M%S'  # e.g., 2024-04-22-15-30-25\n",
    "# Current timestamp\n",
    "current_timestamp = datetime.now().strftime(timestamp_format)\n",
    "# define run Job Name\n",
    "run_name = f\"{BASE_MODEL_ID}-{current_timestamp}\".replace('-', '').replace('/', '').replace('.', '')\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'train_dataset_path': '/opt/ml/input/data/training/train_dataset.json', # path where sagemaker will save training dataset\n",
    "  'eval_dataset_path': '/opt/ml/input/data/validation/eval_dataset.json', # path where sagemaker will save validation dataset\n",
    "  'model_id': BASE_MODEL_ID,           \n",
    "  'dataset_text_field':DATASET_TEXT_FIELD,\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch_fused\",                      # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'learning_rate': 2e-4,                             # learning rate, based on QLoRA paper\n",
    "  'bf16': True,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  #'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'report_to': \"wandb\",                              # report metrics to wandb\n",
    "  'run_name':  run_name\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. Amazon SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_sft.py',    # train script\n",
    "    source_dir           = 'scripts',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment          = {\n",
    "                            \"AWS_REGION\": AWS_REGION,\n",
    "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "                            \"HF_TOKEN\": HF_TOKEN, # huggingface token to access gated models, e.g. llama 2\n",
    "                            \"WANDB_API\": WANDB_API,\n",
    "                            \"WANDB_PROJECT\": 'mistral-tuning-sagemaker',\n",
    "                            \"WANDB_NOTEBOOK_NAME\": 'zmistral-tuning-sagemaker'\n",
    "                            },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch training\n",
    "\n",
    "The \"data\" object indicates the channels where the data will be uploaded\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-24 15:43:55 Starting - Starting the training job...\n",
      "2024-04-24 15:44:20 Pending - Preparing the instances for training.\n",
      "2024-04-24 15:45:13 Downloading - Downloading the training image......\n",
      "2024-04-24 15:48:09 Training - Training image download completed. Training in progress....bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-04-24 15:48:47,457 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-04-24 15:48:47,475 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-24 15:48:47,485 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-04-24 15:48:47,487 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-04-24 15:48:48,935 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.38.2 (from -r requirements.txt (line 1))\n",
      "Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.7/130.7 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting datasets==2.18.0 (from -r requirements.txt (line 2))\n",
      "Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.27.2 (from -r requirements.txt (line 3))\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.42.0)\n",
      "Collecting trl==0.8.6 (from -r requirements.txt (line 6))\n",
      "Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft==0.8.2 (from -r requirements.txt (line 7))\n",
      "Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting flash-attn==2.5.6 (from -r requirements.txt (line 8))\n",
      "Downloading flash_attn-2.5.6.tar.gz (2.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 107.6 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting wandb (from -r requirements.txt (line 9))\n",
      "Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.9.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 5)) (1.11.3)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.6->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (1.11.1.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
      "Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 9))\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting setproctitle (from wandb->-r requirements.txt (line 9))\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (68.1.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 9))\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 9)) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 6)) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 6)) (13.6.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 6)) (1.6.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
      "Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 6)) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 6)) (0.1.0)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 137.4 MB/s eta 0:00:00\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 58.0 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.0/280.0 kB 49.5 MB/s eta 0:00:00\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 32.6 MB/s eta 0:00:00\n",
      "Downloading peft-0.8.2-py3-none-any.whl (183 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.4/183.4 kB 32.5 MB/s eta 0:00:00\n",
      "Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 107.0 MB/s eta 0:00:00\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 20.2 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 267.1/267.1 kB 39.4 MB/s eta 0:00:00\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "Building wheel for flash-attn (setup.py): started\n",
      "Building wheel for flash-attn (setup.py): finished with status 'done'\n",
      "Created wheel for flash-attn: filename=flash_attn-2.5.6-cp310-cp310-linux_x86_64.whl size=120352136 sha256=63ab5a3883b67719671e154beb91522e2901cbe4af0c5e031306a06a85df0be5\n",
      "Stored in directory: /root/.cache/pip/wheels/a8/1c/88/b959d6818b98a46d61ba231683abb7523b89ac1a7ed1e0c206\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, flash-attn, accelerate, wandb, transformers, datasets, trl, peft\n",
      "Attempting uninstall: flash-attn\n",
      "Found existing installation: flash-attn 2.3.6\n",
      "Uninstalling flash-attn-2.3.6:\n",
      "Successfully uninstalled flash-attn-2.3.6\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.25.0\n",
      "Uninstalling accelerate-0.25.0:\n",
      "Successfully uninstalled accelerate-0.25.0\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.36.0\n",
      "Uninstalling transformers-4.36.0:\n",
      "Successfully uninstalled transformers-4.36.0\n",
      "Attempting uninstall: datasets\n",
      "Found existing installation: datasets 2.15.0\n",
      "Uninstalling datasets-2.15.0:\n",
      "Successfully uninstalled datasets-2.15.0\n",
      "Attempting uninstall: trl\n",
      "Found existing installation: trl 0.7.4\n",
      "Uninstalling trl-0.7.4:\n",
      "Successfully uninstalled trl-0.7.4\n",
      "Attempting uninstall: peft\n",
      "Found existing installation: peft 0.7.1\n",
      "Uninstalling peft-0.7.1:\n",
      "Successfully uninstalled peft-0.7.1\n",
      "Successfully installed GitPython-3.1.43 accelerate-0.27.2 appdirs-1.4.4 datasets-2.18.0 docker-pycreds-0.4.0 flash-attn-2.5.6 gitdb-4.0.11 peft-0.8.2 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 transformers-4.38.2 trl-0.8.6 wandb-0.16.6\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2024-04-24 15:49:10,041 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-04-24 15:49:10,041 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-04-24 15:49:10,084 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-24 15:49:10,119 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-24 15:49:10,154 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-04-24 15:49:10,167 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_text_field\": \"text\",\n",
      "        \"eval_dataset_path\": \"/opt/ml/input/data/validation/eval_dataset.json\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_len\": 3072,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"optim\": \"adamw_torch_fused\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"wandb\",\n",
      "        \"run_name\": \"mistralaiMistral7BInstructv01\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"train_dataset_path\": \"/opt/ml/input/data/training/train_dataset.json\",\n",
      "        \"use_qlora\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-510357808667/mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_sft.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"bf16\":true,\"dataset_text_field\":\"text\",\"eval_dataset_path\":\"/opt/ml/input/data/validation/eval_dataset.json\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"mistralai/Mistral-7B-Instruct-v0.1\",\"num_train_epochs\":3,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"wandb\",\"run_name\":\"mistralaiMistral7BInstructv01\",\"save_strategy\":\"epoch\",\"tf32\":true,\"train_dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"use_qlora\":true,\"warmup_ratio\":0.03}\n",
      "SM_USER_ENTRY_POINT=run_sft.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\",\"validation\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_sft\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-510357808667/mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_text_field\":\"text\",\"eval_dataset_path\":\"/opt/ml/input/data/validation/eval_dataset.json\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"mistralai/Mistral-7B-Instruct-v0.1\",\"num_train_epochs\":3,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"wandb\",\"run_name\":\"mistralaiMistral7BInstructv01\",\"save_strategy\":\"epoch\",\"tf32\":true,\"train_dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"use_qlora\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-510357808667/mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111/source/sourcedir.tar.gz\",\"module_name\":\"run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_sft.py\"}\n",
      "SM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_text_field\",\"text\",\"--eval_dataset_path\",\"/opt/ml/input/data/validation/eval_dataset.json\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_len\",\"3072\",\"--merge_adapters\",\"True\",\"--model_id\",\"mistralai/Mistral-7B-Instruct-v0.1\",\"--num_train_epochs\",\"3\",\"--optim\",\"adamw_torch_fused\",\"--output_dir\",\"/tmp/tun\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"wandb\",\"--run_name\",\"mistralaiMistral7BInstructv01\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--train_dataset_path\",\"/opt/ml/input/data/training/train_dataset.json\",\"--use_qlora\",\"True\",\"--warmup_ratio\",\"0.03\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "SM_HP_BF16=true\n",
      "SM_HP_DATASET_TEXT_FIELD=text\n",
      "SM_HP_EVAL_DATASET_PATH=/opt/ml/input/data/validation/eval_dataset.json\n",
      "SM_HP_GRADIENT_ACCUMULATION_STEPS=4\n",
      "SM_HP_GRADIENT_CHECKPOINTING=true\n",
      "SM_HP_LEARNING_RATE=0.0002\n",
      "SM_HP_LOGGING_STEPS=10\n",
      "SM_HP_LR_SCHEDULER_TYPE=constant\n",
      "SM_HP_MAX_GRAD_NORM=0.3\n",
      "SM_HP_MAX_SEQ_LEN=3072\n",
      "SM_HP_MERGE_ADAPTERS=true\n",
      "SM_HP_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1\n",
      "SM_HP_NUM_TRAIN_EPOCHS=3\n",
      "SM_HP_OPTIM=adamw_torch_fused\n",
      "SM_HP_OUTPUT_DIR=/tmp/tun\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "SM_HP_REPORT_TO=wandb\n",
      "SM_HP_RUN_NAME=mistralaiMistral7BInstructv01\n",
      "SM_HP_SAVE_STRATEGY=epoch\n",
      "SM_HP_TF32=true\n",
      "SM_HP_TRAIN_DATASET_PATH=/opt/ml/input/data/training/train_dataset.json\n",
      "SM_HP_USE_QLORA=true\n",
      "SM_HP_WARMUP_RATIO=0.03\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.10 run_sft.py --bf16 True --dataset_text_field text --eval_dataset_path /opt/ml/input/data/validation/eval_dataset.json --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_len 3072 --merge_adapters True --model_id mistralai/Mistral-7B-Instruct-v0.1 --num_train_epochs 3 --optim adamw_torch_fused --output_dir /tmp/tun --per_device_train_batch_size 1 --report_to wandb --run_name mistralaiMistral7BInstructv01 --save_strategy epoch --tf32 True --train_dataset_path /opt/ml/input/data/training/train_dataset.json --use_qlora True --warmup_ratio 0.03\n",
      "2024-04-24 15:49:10,168 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2024-04-24 15:49:10,168 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 2143 examples [00:00, 177646.77 examples/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 535 examples [00:00, 129678.26 examples/s]\n",
      "Using QLoRA\n",
      "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 6.44MB/s]\n",
      "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]\n",
      "model.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 152MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\n",
      "model-00001-of-00002.safetensors:   0%|          | 31.5M/9.94G [00:00<00:32, 308MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   1%|          | 73.4M/9.94G [00:00<00:30, 328MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   1%|          | 115M/9.94G [00:00<00:27, 351MB/s] #033[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 157M/9.94G [00:00<00:30, 325MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 199M/9.94G [00:00<00:57, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 252M/9.94G [00:01<00:43, 222MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 294M/9.94G [00:01<00:37, 255MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 336M/9.94G [00:01<00:33, 289MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 377M/9.94G [00:01<00:30, 311MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 419M/9.94G [00:01<00:31, 305MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   5%|▍         | 461M/9.94G [00:01<00:31, 301MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 503M/9.94G [00:01<00:29, 325MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 545M/9.94G [00:01<00:28, 326MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 598M/9.94G [00:02<00:26, 358MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   6%|▋         | 640M/9.94G [00:02<00:26, 357MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 682M/9.94G [00:02<00:28, 325MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 724M/9.94G [00:02<00:28, 325MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 765M/9.94G [00:02<00:28, 326MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 807M/9.94G [00:02<00:26, 345MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   9%|▊         | 849M/9.94G [00:02<00:25, 354MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 891M/9.94G [00:02<00:24, 364MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 933M/9.94G [00:02<00:24, 369MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  10%|▉         | 975M/9.94G [00:03<00:25, 353MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  10%|█         | 1.03G/9.94G [00:03<00:23, 375MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  11%|█         | 1.08G/9.94G [00:03<00:22, 398MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  11%|█▏        | 1.12G/9.94G [00:03<00:21, 403MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 1.16G/9.94G [00:03<00:22, 395MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 1.21G/9.94G [00:03<00:21, 400MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 1.26G/9.94G [00:03<00:21, 397MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 1.30G/9.94G [00:03<00:22, 384MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 1.34G/9.94G [00:04<00:22, 379MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 1.38G/9.94G [00:04<00:22, 377MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 1.44G/9.94G [00:04<00:20, 412MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  15%|█▍        | 1.49G/9.94G [00:04<00:19, 424MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 1.54G/9.94G [00:04<00:19, 439MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 1.59G/9.94G [00:04<00:18, 453MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 1.65G/9.94G [00:04<00:18, 457MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 1.70G/9.94G [00:04<00:18, 450MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 1.75G/9.94G [00:04<00:19, 414MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 1.80G/9.94G [00:05<00:19, 421MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  19%|█▊        | 1.86G/9.94G [00:05<00:18, 442MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 1.91G/9.94G [00:05<00:18, 440MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 1.96G/9.94G [00:05<00:22, 352MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  20%|██        | 2.01G/9.94G [00:05<00:20, 381MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  21%|██        | 2.07G/9.94G [00:05<00:19, 404MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  21%|██▏       | 2.12G/9.94G [00:05<00:18, 425MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 2.17G/9.94G [00:06<00:18, 411MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 2.22G/9.94G [00:06<00:18, 411MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 2.28G/9.94G [00:06<00:18, 423MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 2.33G/9.94G [00:06<00:17, 437MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 2.38G/9.94G [00:06<00:16, 452MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 2.43G/9.94G [00:06<00:16, 448MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 2.49G/9.94G [00:06<00:17, 422MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 2.55G/9.94G [00:06<00:15, 464MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 2.60G/9.94G [00:06<00:16, 442MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 2.65G/9.94G [00:07<00:16, 447MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 2.71G/9.94G [00:07<00:16, 445MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 2.76G/9.94G [00:07<00:17, 414MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 2.81G/9.94G [00:07<00:17, 419MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 2.86G/9.94G [00:07<00:16, 420MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 2.92G/9.94G [00:07<00:16, 436MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 2.97G/9.94G [00:07<00:15, 446MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  30%|███       | 3.02G/9.94G [00:07<00:15, 457MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  31%|███       | 3.07G/9.94G [00:08<00:15, 431MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  31%|███▏      | 3.12G/9.94G [00:08<00:16, 411MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 3.17G/9.94G [00:08<00:16, 410MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 3.21G/9.94G [00:08<00:17, 395MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 3.25G/9.94G [00:08<00:19, 350MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 3.29G/9.94G [00:08<00:19, 348MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  34%|███▎      | 3.34G/9.94G [00:08<00:16, 390MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 3.40G/9.94G [00:08<00:16, 401MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  35%|███▍      | 3.44G/9.94G [00:09<00:16, 404MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  35%|███▌      | 3.49G/9.94G [00:09<00:15, 414MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  36%|███▌      | 3.54G/9.94G [00:09<00:15, 421MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  36%|███▌      | 3.60G/9.94G [00:09<00:16, 379MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 3.64G/9.94G [00:09<00:16, 379MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 3.68G/9.94G [00:09<00:17, 368MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 3.73G/9.94G [00:09<00:15, 403MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 3.77G/9.94G [00:09<00:15, 388MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 3.82G/9.94G [00:09<00:15, 392MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 3.86G/9.94G [00:10<00:22, 266MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 3.90G/9.94G [00:10<00:21, 284MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 3.95G/9.94G [00:10<00:20, 298MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  40%|████      | 4.00G/9.94G [00:10<00:20, 289MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  41%|████      | 4.06G/9.94G [00:10<00:16, 349MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  41%|████      | 4.10G/9.94G [00:10<00:17, 328MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 4.14G/9.94G [00:11<00:17, 340MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 4.19G/9.94G [00:11<00:15, 368MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 4.24G/9.94G [00:11<00:15, 378MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 4.29G/9.94G [00:11<00:14, 400MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  44%|████▎     | 4.33G/9.94G [00:11<00:13, 405MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 4.37G/9.94G [00:11<00:13, 398MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 4.42G/9.94G [00:11<00:13, 411MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  45%|████▌     | 4.48G/9.94G [00:11<00:12, 440MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 4.53G/9.94G [00:11<00:12, 439MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 4.58G/9.94G [00:12<00:12, 434MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 4.63G/9.94G [00:12<00:12, 411MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 4.69G/9.94G [00:12<00:12, 419MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 4.74G/9.94G [00:12<00:12, 413MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 4.79G/9.94G [00:12<00:11, 436MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  49%|████▊     | 4.84G/9.94G [00:12<00:12, 406MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  49%|████▉     | 4.89G/9.94G [00:12<00:12, 404MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  50%|████▉     | 4.93G/9.94G [00:12<00:12, 398MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  50%|████▉     | 4.97G/9.94G [00:13<00:12, 383MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  51%|█████     | 5.02G/9.94G [00:13<00:12, 400MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  51%|█████     | 5.08G/9.94G [00:13<00:11, 425MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 5.13G/9.94G [00:13<00:11, 426MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 5.18G/9.94G [00:13<00:10, 444MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 5.23G/9.94G [00:13<00:11, 420MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 5.30G/9.94G [00:13<00:10, 451MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▍    | 5.35G/9.94G [00:13<00:11, 417MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▍    | 5.40G/9.94G [00:14<00:10, 435MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▍    | 5.45G/9.94G [00:14<00:10, 441MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▌    | 5.51G/9.94G [00:14<00:10, 434MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▌    | 5.56G/9.94G [00:14<00:10, 434MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▋    | 5.61G/9.94G [00:14<00:09, 445MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  57%|█████▋    | 5.66G/9.94G [00:14<00:09, 429MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  57%|█████▋    | 5.71G/9.94G [00:14<00:10, 411MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 5.76G/9.94G [00:14<00:10, 412MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 5.81G/9.94G [00:15<00:09, 416MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 5.85G/9.94G [00:15<00:10, 405MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 5.89G/9.94G [00:15<00:09, 408MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  60%|█████▉    | 5.93G/9.94G [00:15<00:10, 371MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  60%|██████    | 5.98G/9.94G [00:15<00:12, 329MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  61%|██████    | 6.03G/9.94G [00:15<00:10, 362MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  61%|██████    | 6.07G/9.94G [00:15<00:10, 363MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 6.12G/9.94G [00:15<00:09, 395MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 6.18G/9.94G [00:16<00:09, 409MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 6.23G/9.94G [00:16<00:08, 418MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 6.28G/9.94G [00:16<00:08, 427MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▎   | 6.33G/9.94G [00:16<00:09, 382MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▍   | 6.38G/9.94G [00:16<00:09, 378MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▍   | 6.42G/9.94G [00:16<00:11, 308MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▍   | 6.46G/9.94G [00:16<00:14, 245MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▌   | 6.49G/9.94G [00:17<00:14, 231MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 6.52G/9.94G [00:17<00:15, 220MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 6.55G/9.94G [00:17<00:15, 221MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 6.59G/9.94G [00:17<00:15, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 6.62G/9.94G [00:17<00:15, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 6.65G/9.94G [00:17<00:15, 209MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 6.68G/9.94G [00:18<00:15, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.94G [00:18<00:15, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 6.74G/9.94G [00:18<00:15, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 6.77G/9.94G [00:18<00:15, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 6.81G/9.94G [00:18<00:15, 205MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 6.84G/9.94G [00:18<00:15, 206MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 6.86G/9.94G [00:18<00:15, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 6.89G/9.94G [00:19<00:14, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|██████▉   | 6.92G/9.94G [00:19<00:14, 209MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|██████▉   | 6.94G/9.94G [00:19<00:14, 208MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|███████   | 6.96G/9.94G [00:19<00:14, 202MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  70%|███████   | 6.99G/9.94G [00:19<00:14, 209MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 7.01G/9.94G [00:19<00:14, 209MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 7.04G/9.94G [00:19<00:14, 202MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 7.07G/9.94G [00:19<00:13, 207MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  71%|███████▏  | 7.09G/9.94G [00:20<00:13, 206MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 7.11G/9.94G [00:20<00:14, 199MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 7.13G/9.94G [00:20<00:20, 140MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 7.18G/9.94G [00:20<00:13, 209MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 7.21G/9.94G [00:20<00:11, 228MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 7.25G/9.94G [00:20<00:12, 218MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 7.28G/9.94G [00:20<00:12, 217MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▎  | 7.31G/9.94G [00:21<00:12, 214MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 7.34G/9.94G [00:21<00:12, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 7.37G/9.94G [00:21<00:12, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 7.40G/9.94G [00:21<00:12, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 7.43G/9.94G [00:21<00:12, 206MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 7.46G/9.94G [00:21<00:12, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▌  | 7.49G/9.94G [00:22<00:11, 207MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 7.52G/9.94G [00:22<00:11, 205MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 7.55G/9.94G [00:22<00:11, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 7.58G/9.94G [00:22<00:11, 205MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▋  | 7.60G/9.94G [00:22<00:11, 205MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.62G/9.94G [00:22<00:11, 206MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.64G/9.94G [00:22<00:11, 203MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.68G/9.94G [00:22<00:10, 210MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 7.70G/9.94G [00:23<00:11, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.73G/9.94G [00:23<00:10, 208MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.75G/9.94G [00:23<00:10, 203MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.77G/9.94G [00:23<00:10, 203MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 7.80G/9.94G [00:23<00:10, 208MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▊  | 7.82G/9.94G [00:23<00:10, 206MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 7.85G/9.94G [00:23<00:10, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 7.89G/9.94G [00:23<00:09, 207MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 7.91G/9.94G [00:24<00:09, 204MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 7.93G/9.94G [00:24<00:10, 198MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|████████  | 7.96G/9.94G [00:24<00:10, 193MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|████████  | 7.98G/9.94G [00:24<00:10, 188MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  80%|████████  | 8.00G/9.94G [00:24<00:10, 191MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 8.02G/9.94G [00:24<00:10, 191MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 8.04G/9.94G [00:24<00:10, 189MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 8.06G/9.94G [00:24<00:09, 188MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  81%|████████▏ | 8.08G/9.94G [00:25<00:09, 187MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.11G/9.94G [00:25<00:10, 182MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.13G/9.94G [00:25<00:10, 181MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.15G/9.94G [00:25<00:09, 184MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.17G/9.94G [00:25<00:09, 184MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 8.19G/9.94G [00:25<00:09, 181MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.21G/9.94G [00:25<00:09, 185MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.23G/9.94G [00:25<00:09, 181MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.25G/9.94G [00:25<00:09, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.27G/9.94G [00:26<00:09, 179MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 8.29G/9.94G [00:26<00:08, 184MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▎ | 8.32G/9.94G [00:26<00:08, 191MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.34G/9.94G [00:26<00:08, 183MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.36G/9.94G [00:26<00:08, 185MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.38G/9.94G [00:26<00:08, 185MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.94G [00:26<00:08, 191MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▍ | 8.42G/9.94G [00:26<00:08, 183MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▍ | 8.44G/9.94G [00:26<00:08, 184MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 8.46G/9.94G [00:27<00:08, 184MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 8.48G/9.94G [00:27<00:08, 177MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 8.51G/9.94G [00:27<00:07, 190MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 8.54G/9.94G [00:27<00:07, 179MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 8.56G/9.94G [00:27<00:07, 177MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▋ | 8.58G/9.94G [00:27<00:07, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▋ | 8.60G/9.94G [00:27<00:07, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.62G/9.94G [00:27<00:07, 184MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.64G/9.94G [00:28<00:07, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.66G/9.94G [00:28<00:07, 175MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 8.68G/9.94G [00:28<00:07, 178MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.70G/9.94G [00:28<00:06, 180MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.72G/9.94G [00:28<00:07, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.75G/9.94G [00:28<00:06, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.77G/9.94G [00:28<00:06, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 8.79G/9.94G [00:28<00:06, 179MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▊ | 8.81G/9.94G [00:29<00:06, 177MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.83G/9.94G [00:29<00:06, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.85G/9.94G [00:29<00:06, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.87G/9.94G [00:29<00:06, 178MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 8.89G/9.94G [00:29<00:05, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|████████▉ | 8.91G/9.94G [00:29<00:05, 178MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|████████▉ | 8.93G/9.94G [00:29<00:05, 180MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 8.95G/9.94G [00:29<00:05, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 8.98G/9.94G [00:30<00:05, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 9.00G/9.94G [00:30<00:05, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 9.02G/9.94G [00:30<00:05, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 9.04G/9.94G [00:30<00:05, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 9.06G/9.94G [00:30<00:07, 118MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.11G/9.94G [00:30<00:04, 195MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.14G/9.94G [00:30<00:03, 203MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 9.18G/9.94G [00:31<00:04, 185MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.21G/9.94G [00:31<00:04, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.23G/9.94G [00:31<00:04, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.25G/9.94G [00:31<00:04, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.27G/9.94G [00:31<00:03, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 9.29G/9.94G [00:31<00:03, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▎| 9.31G/9.94G [00:31<00:03, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.33G/9.94G [00:32<00:03, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.35G/9.94G [00:32<00:03, 165MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.37G/9.94G [00:32<00:03, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 9.40G/9.94G [00:32<00:03, 176MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▍| 9.42G/9.94G [00:32<00:03, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▍| 9.44G/9.94G [00:32<00:02, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▌| 9.46G/9.94G [00:32<00:02, 165MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▌| 9.49G/9.94G [00:33<00:02, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.51G/9.94G [00:33<00:02, 173MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.53G/9.94G [00:33<00:02, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 9.55G/9.94G [00:33<00:02, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▋| 9.57G/9.94G [00:33<00:02, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▋| 9.59G/9.94G [00:33<00:02, 169MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.62G/9.94G [00:33<00:01, 165MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.64G/9.94G [00:33<00:01, 167MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.66G/9.94G [00:34<00:01, 166MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 9.68G/9.94G [00:34<00:01, 170MB/s]\n",
      "#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.70G/9.94G [00:34<00:01, 174MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.72G/9.94G [00:34<00:01, 164MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.74G/9.94G [00:34<00:01, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.76G/9.94G [00:34<00:01, 163MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 9.78G/9.94G [00:34<00:00, 166MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▊| 9.80G/9.94G [00:34<00:00, 168MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 9.83G/9.94G [00:35<00:00, 161MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 9.86G/9.94G [00:35<00:00, 166MB/s]#033[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 9.88G/9.94G [00:35<00:00, 170MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 9.90G/9.94G [00:35<00:00, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 9.92G/9.94G [00:35<00:00, 172MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 9.94G/9.94G [00:35<00:00, 171MB/s]#033[A\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 9.94G/9.94G [00:35<00:00, 278MB/s]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:35<00:35, 35.82s/it]\n",
      "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\n",
      "model-00002-of-00002.safetensors:   1%|          | 31.5M/4.54G [00:00<00:15, 299MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   1%|▏         | 62.9M/4.54G [00:00<00:21, 211MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   2%|▏         | 94.4M/4.54G [00:00<00:23, 191MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   3%|▎         | 115M/4.54G [00:00<00:23, 186MB/s] #033[A\n",
      "model-00002-of-00002.safetensors:   3%|▎         | 136M/4.54G [00:00<00:24, 183MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   3%|▎         | 157M/4.54G [00:00<00:24, 181MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   4%|▍         | 178M/4.54G [00:00<00:24, 178MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   4%|▍         | 199M/4.54G [00:01<00:24, 178MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   5%|▍         | 220M/4.54G [00:01<00:25, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   5%|▌         | 241M/4.54G [00:01<00:25, 170MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   6%|▌         | 262M/4.54G [00:01<00:25, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   6%|▌         | 283M/4.54G [00:01<00:24, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   7%|▋         | 304M/4.54G [00:01<00:23, 181MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   7%|▋         | 336M/4.54G [00:01<00:20, 206MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   8%|▊         | 367M/4.54G [00:01<00:18, 223MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   9%|▉         | 398M/4.54G [00:02<00:17, 233MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:   9%|▉         | 430M/4.54G [00:02<00:17, 240MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  10%|█         | 461M/4.54G [00:02<00:16, 249MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  11%|█         | 493M/4.54G [00:02<00:15, 254MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  12%|█▏        | 524M/4.54G [00:02<00:15, 253MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  12%|█▏        | 556M/4.54G [00:02<00:15, 259MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  13%|█▎        | 587M/4.54G [00:02<00:15, 261MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  14%|█▎        | 619M/4.54G [00:02<00:15, 257MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  14%|█▍        | 650M/4.54G [00:02<00:14, 261MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  15%|█▌        | 682M/4.54G [00:03<00:14, 259MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  16%|█▌        | 713M/4.54G [00:03<00:14, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  16%|█▋        | 744M/4.54G [00:03<00:14, 260MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  17%|█▋        | 776M/4.54G [00:03<00:14, 262MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  18%|█▊        | 807M/4.54G [00:03<00:14, 265MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  18%|█▊        | 839M/4.54G [00:03<00:14, 264MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  19%|█▉        | 870M/4.54G [00:03<00:13, 264MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  20%|█▉        | 902M/4.54G [00:03<00:14, 258MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  21%|██        | 933M/4.54G [00:04<00:13, 264MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  21%|██        | 965M/4.54G [00:04<00:13, 259MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  22%|██▏       | 996M/4.54G [00:04<00:13, 260MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  23%|██▎       | 1.03G/4.54G [00:04<00:13, 261MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  23%|██▎       | 1.06G/4.54G [00:04<00:13, 266MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  24%|██▍       | 1.09G/4.54G [00:04<00:13, 258MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  25%|██▍       | 1.12G/4.54G [00:04<00:14, 230MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  25%|██▌       | 1.15G/4.54G [00:05<00:19, 170MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  27%|██▋       | 1.21G/4.54G [00:05<00:14, 224MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  27%|██▋       | 1.24G/4.54G [00:05<00:16, 194MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  28%|██▊       | 1.27G/4.54G [00:05<00:19, 170MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  28%|██▊       | 1.29G/4.54G [00:05<00:19, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  29%|██▉       | 1.31G/4.54G [00:06<00:19, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  29%|██▉       | 1.33G/4.54G [00:06<00:19, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  30%|██▉       | 1.35G/4.54G [00:06<00:19, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  30%|███       | 1.37G/4.54G [00:06<00:18, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  31%|███       | 1.39G/4.54G [00:06<00:19, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  31%|███       | 1.42G/4.54G [00:06<00:19, 160MB/s]\n",
      "#033[A\n",
      "model-00002-of-00002.safetensors:  32%|███▏      | 1.44G/4.54G [00:06<00:18, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  32%|███▏      | 1.46G/4.54G [00:06<00:18, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  33%|███▎      | 1.48G/4.54G [00:07<00:18, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  33%|███▎      | 1.50G/4.54G [00:07<00:18, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  33%|███▎      | 1.52G/4.54G [00:07<00:18, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  34%|███▍      | 1.54G/4.54G [00:07<00:18, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  34%|███▍      | 1.56G/4.54G [00:07<00:17, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  35%|███▍      | 1.58G/4.54G [00:07<00:18, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  35%|███▌      | 1.60G/4.54G [00:07<00:17, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  36%|███▌      | 1.63G/4.54G [00:07<00:18, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  36%|███▋      | 1.65G/4.54G [00:08<00:17, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  37%|███▋      | 1.67G/4.54G [00:08<00:17, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  37%|███▋      | 1.69G/4.54G [00:08<00:17, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  38%|███▊      | 1.71G/4.54G [00:08<00:17, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  38%|███▊      | 1.73G/4.54G [00:08<00:17, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  39%|███▊      | 1.75G/4.54G [00:08<00:18, 151MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  39%|███▉      | 1.78G/4.54G [00:08<00:16, 169MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  40%|███▉      | 1.80G/4.54G [00:09<00:16, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  40%|████      | 1.82G/4.54G [00:09<00:16, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  41%|████      | 1.85G/4.54G [00:09<00:16, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  41%|████      | 1.87G/4.54G [00:09<00:15, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  42%|████▏     | 1.89G/4.54G [00:09<00:16, 158MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  42%|████▏     | 1.91G/4.54G [00:09<00:16, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  42%|████▏     | 1.93G/4.54G [00:09<00:16, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  43%|████▎     | 1.95G/4.54G [00:09<00:16, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  43%|████▎     | 1.97G/4.54G [00:10<00:15, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  44%|████▍     | 1.99G/4.54G [00:10<00:15, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  44%|████▍     | 2.01G/4.54G [00:10<00:15, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  45%|████▍     | 2.03G/4.54G [00:10<00:15, 158MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  45%|████▌     | 2.06G/4.54G [00:10<00:15, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  46%|████▌     | 2.08G/4.54G [00:10<00:15, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  46%|████▌     | 2.10G/4.54G [00:10<00:15, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  47%|████▋     | 2.12G/4.54G [00:10<00:15, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  47%|████▋     | 2.14G/4.54G [00:11<00:15, 156MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  48%|████▊     | 2.16G/4.54G [00:11<00:15, 151MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  48%|████▊     | 2.18G/4.54G [00:11<00:16, 144MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  48%|████▊     | 2.20G/4.54G [00:11<00:16, 140MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  49%|████▉     | 2.22G/4.54G [00:11<00:16, 139MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  49%|████▉     | 2.24G/4.54G [00:11<00:16, 135MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  50%|████▉     | 2.26G/4.54G [00:12<00:17, 132MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  50%|█████     | 2.29G/4.54G [00:12<00:17, 132MB/s]\n",
      "#033[A\n",
      "model-00002-of-00002.safetensors:  51%|█████     | 2.31G/4.54G [00:12<00:16, 132MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  51%|█████▏    | 2.33G/4.54G [00:12<00:16, 132MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  52%|█████▏    | 2.35G/4.54G [00:12<00:16, 132MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  52%|█████▏    | 2.37G/4.54G [00:12<00:16, 136MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  53%|█████▎    | 2.39G/4.54G [00:12<00:14, 143MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  53%|█████▎    | 2.41G/4.54G [00:13<00:13, 156MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  54%|█████▎    | 2.43G/4.54G [00:13<00:13, 156MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  54%|█████▍    | 2.45G/4.54G [00:13<00:12, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  55%|█████▍    | 2.47G/4.54G [00:13<00:13, 157MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  55%|█████▌    | 2.51G/4.54G [00:13<00:12, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  56%|█████▌    | 2.53G/4.54G [00:13<00:12, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  56%|█████▌    | 2.55G/4.54G [00:13<00:11, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  57%|█████▋    | 2.57G/4.54G [00:14<00:11, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  57%|█████▋    | 2.59G/4.54G [00:14<00:11, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  58%|█████▊    | 2.61G/4.54G [00:14<00:10, 176MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  58%|█████▊    | 2.63G/4.54G [00:14<00:10, 179MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  58%|█████▊    | 2.65G/4.54G [00:14<00:11, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  59%|█████▉    | 2.67G/4.54G [00:14<00:10, 173MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  59%|█████▉    | 2.69G/4.54G [00:14<00:10, 174MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  60%|█████▉    | 2.72G/4.54G [00:14<00:10, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  60%|██████    | 2.74G/4.54G [00:14<00:10, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  61%|██████    | 2.76G/4.54G [00:15<00:15, 112MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  62%|██████▏   | 2.81G/4.54G [00:15<00:09, 184MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  63%|██████▎   | 2.84G/4.54G [00:15<00:08, 201MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  63%|██████▎   | 2.87G/4.54G [00:15<00:09, 177MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  64%|██████▍   | 2.90G/4.54G [00:15<00:09, 170MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  64%|██████▍   | 2.93G/4.54G [00:16<00:09, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  65%|██████▍   | 2.95G/4.54G [00:16<00:09, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  65%|██████▌   | 2.97G/4.54G [00:16<00:09, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  66%|██████▌   | 2.99G/4.54G [00:16<00:09, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  66%|██████▋   | 3.01G/4.54G [00:16<00:09, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  67%|██████▋   | 3.03G/4.54G [00:16<00:08, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  67%|██████▋   | 3.05G/4.54G [00:16<00:08, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  68%|██████▊   | 3.07G/4.54G [00:16<00:08, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  68%|██████▊   | 3.09G/4.54G [00:17<00:08, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  69%|██████▊   | 3.11G/4.54G [00:17<00:08, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  69%|██████▉   | 3.14G/4.54G [00:17<00:08, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  70%|██████▉   | 3.16G/4.54G [00:17<00:08, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  70%|██████▉   | 3.18G/4.54G [00:17<00:07, 171MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  70%|███████   | 3.20G/4.54G [00:17<00:08, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  71%|███████   | 3.22G/4.54G [00:17<00:07, 169MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  71%|███████▏  | 3.24G/4.54G [00:18<00:08, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  72%|███████▏  | 3.26G/4.54G [00:18<00:07, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  72%|███████▏  | 3.28G/4.54G [00:18<00:07, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  73%|███████▎  | 3.30G/4.54G [00:18<00:07, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  73%|███████▎  | 3.32G/4.54G [00:18<00:07, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  74%|███████▎  | 3.34G/4.54G [00:18<00:07, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  74%|███████▍  | 3.37G/4.54G [00:18<00:06, 173MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  75%|███████▍  | 3.39G/4.54G [00:18<00:07, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  75%|███████▌  | 3.41G/4.54G [00:19<00:06, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  76%|███████▌  | 3.43G/4.54G [00:19<00:06, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  76%|███████▌  | 3.45G/4.54G [00:19<00:06, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  76%|███████▋  | 3.47G/4.54G [00:19<00:06, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  77%|███████▋  | 3.49G/4.54G [00:19<00:06, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  77%|███████▋  | 3.51G/4.54G [00:19<00:06, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  78%|███████▊  | 3.53G/4.54G [00:19<00:06, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  78%|███████▊  | 3.55G/4.54G [00:19<00:06, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  79%|███████▊  | 3.58G/4.54G [00:20<00:05, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  79%|███████▉  | 3.60G/4.54G [00:20<00:05, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  80%|███████▉  | 3.62G/4.54G [00:20<00:05, 169MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  80%|████████  | 3.64G/4.54G [00:20<00:05, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  81%|████████  | 3.66G/4.54G [00:20<00:05, 168MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  81%|████████  | 3.68G/4.54G [00:20<00:05, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  82%|████████▏ | 3.70G/4.54G [00:20<00:05, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  82%|████████▏ | 3.72G/4.54G [00:20<00:05, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  82%|████████▏ | 3.74G/4.54G [00:21<00:04, 159MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  83%|████████▎ | 3.76G/4.54G [00:21<00:04, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  83%|████████▎ | 3.79G/4.54G [00:21<00:04, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  84%|████████▍ | 3.81G/4.54G [00:21<00:04, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  84%|████████▍ | 3.83G/4.54G [00:21<00:04, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  85%|████████▍ | 3.85G/4.54G [00:21<00:04, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  85%|████████▌ | 3.87G/4.54G [00:21<00:04, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  86%|████████▌ | 3.89G/4.54G [00:21<00:03, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  86%|████████▌ | 3.91G/4.54G [00:22<00:03, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  87%|████████▋ | 3.93G/4.54G [00:22<00:03, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  87%|████████▋ | 3.95G/4.54G [00:22<00:03, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  88%|████████▊ | 3.97G/4.54G [00:22<00:03, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  88%|████████▊ | 4.00G/4.54G [00:22<00:03, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  88%|████████▊ | 4.02G/4.54G [00:22<00:03, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  89%|████████▉ | 4.04G/4.54G [00:22<00:03, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  89%|████████▉ | 4.06G/4.54G [00:22<00:02, 165MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  90%|████████▉ | 4.08G/4.54G [00:23<00:02, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  90%|█████████ | 4.10G/4.54G [00:23<00:02, 166MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  91%|█████████ | 4.12G/4.54G [00:23<00:02, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  91%|█████████ | 4.14G/4.54G [00:23<00:02, 170MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  92%|█████████▏| 4.16G/4.54G [00:23<00:02, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  92%|█████████▏| 4.18G/4.54G [00:23<00:02, 158MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  93%|█████████▎| 4.20G/4.54G [00:23<00:02, 164MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  93%|█████████▎| 4.23G/4.54G [00:23<00:01, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  94%|█████████▎| 4.25G/4.54G [00:24<00:01, 169MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  94%|█████████▍| 4.27G/4.54G [00:24<00:01, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  94%|█████████▍| 4.29G/4.54G [00:24<00:01, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  95%|█████████▍| 4.31G/4.54G [00:24<00:01, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  95%|█████████▌| 4.33G/4.54G [00:24<00:01, 167MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  96%|█████████▌| 4.35G/4.54G [00:24<00:01, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  96%|█████████▋| 4.37G/4.54G [00:24<00:01, 163MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  97%|█████████▋| 4.39G/4.54G [00:25<00:00, 161MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  97%|█████████▋| 4.41G/4.54G [00:25<00:00, 162MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 4.44G/4.54G [00:25<00:00, 160MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  98%|█████████▊| 4.46G/4.54G [00:25<00:00, 119MB/s]#033[A\n",
      "model-00002-of-00002.safetensors:  99%|█████████▉| 4.51G/4.54G [00:25<00:00, 198MB/s]#033[A\n",
      "model-00002-of-00002.safetensors: 100%|█████████▉| 4.54G/4.54G [00:25<00:00, 172MB/s]#033[A\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 4.54G/4.54G [00:25<00:00, 175MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:01<00:00, 30.05s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:01<00:00, 30.92s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.24s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.49s/it]\n",
      "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\n",
      "generation_config.json: 100%|██████████| 116/116 [00:00<00:00, 1.31MB/s]\n",
      "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 16.8MB/s]\n",
      "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.4MB/s]\n",
      "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 27.2MB/s]\n",
      "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 793kB/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 1 examples [00:00,  1.14 examples/s]\n",
      "Generating train split: 211 examples [00:00, 294.25 examples/s]\n",
      "Generating train split: 436 examples [00:01, 607.01 examples/s]\n",
      "Generating train split: 436 examples [00:01, 386.34 examples/s]\n",
      "Map:   0%|          | 0/535 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 535/535 [00:00<00:00, 1045.49 examples/s]\n",
      "Map: 100%|██████████| 535/535 [00:00<00:00, 1037.96 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "wandb: Currently logged in as: danielpradilla. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.16.6\n",
      "wandb: Run data is saved locally in /opt/ml/code/wandb/run-20240424_155039-mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111-pzfyaf-algo-1\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run mistralaiMistral7BInstructv01\n",
      "wandb: ⭐️ View project at https://wandb.ai/danielpradilla/mistral-tuning-sagemaker\n",
      "wandb: 🚀 View run at https://wandb.ai/danielpradilla/mistral-tuning-sagemaker/runs/mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111-pzfyaf-algo-1\n",
      "0%|          | 0/327 [00:00<?, ?it/s]\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "0%|          | 1/327 [00:19<1:48:39, 20.00s/it]\n",
      "1%|          | 2/327 [00:39<1:45:57, 19.56s/it]\n",
      "1%|          | 3/327 [00:58<1:45:05, 19.46s/it]\n",
      "1%|          | 4/327 [01:17<1:44:20, 19.38s/it]\n",
      "2%|▏         | 5/327 [01:37<1:43:46, 19.34s/it]\n",
      "2%|▏         | 6/327 [01:56<1:43:17, 19.31s/it]\n",
      "2%|▏         | 7/327 [02:15<1:43:04, 19.33s/it]\n",
      "2%|▏         | 8/327 [02:34<1:42:37, 19.30s/it]\n",
      "3%|▎         | 9/327 [02:54<1:42:13, 19.29s/it]\n",
      "3%|▎         | 10/327 [03:13<1:42:03, 19.32s/it]\n",
      "{'loss': 2.4921, 'grad_norm': 0.291015625, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "3%|▎         | 10/327 [03:13<1:42:03, 19.32s/it]\n",
      "3%|▎         | 11/327 [03:32<1:41:38, 19.30s/it]\n",
      "4%|▎         | 12/327 [03:52<1:41:14, 19.28s/it]\n",
      "4%|▍         | 13/327 [04:11<1:41:03, 19.31s/it]\n",
      "4%|▍         | 14/327 [04:30<1:40:38, 19.29s/it]\n",
      "5%|▍         | 15/327 [04:50<1:40:15, 19.28s/it]\n",
      "5%|▍         | 16/327 [05:09<1:40:01, 19.30s/it]\n",
      "5%|▌         | 17/327 [05:28<1:39:39, 19.29s/it]\n",
      "6%|▌         | 18/327 [05:47<1:39:16, 19.28s/it]\n",
      "6%|▌         | 19/327 [06:07<1:38:55, 19.27s/it]\n",
      "6%|▌         | 20/327 [06:26<1:38:43, 19.29s/it]\n",
      "{'loss': 2.3256, 'grad_norm': 0.345703125, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "6%|▌         | 20/327 [06:26<1:38:43, 19.29s/it]\n",
      "6%|▋         | 21/327 [06:45<1:38:20, 19.28s/it]\n",
      "7%|▋         | 22/327 [07:04<1:37:58, 19.27s/it]\n",
      "7%|▋         | 23/327 [07:24<1:37:46, 19.30s/it]\n",
      "7%|▋         | 24/327 [07:43<1:37:22, 19.28s/it]\n",
      "8%|▊         | 25/327 [08:02<1:37:01, 19.28s/it]\n",
      "8%|▊         | 26/327 [08:22<1:36:51, 19.31s/it]\n",
      "8%|▊         | 27/327 [08:41<1:36:27, 19.29s/it]\n",
      "9%|▊         | 28/327 [09:00<1:36:05, 19.28s/it]\n",
      "9%|▉         | 29/327 [09:20<1:35:51, 19.30s/it]\n",
      "9%|▉         | 30/327 [09:39<1:35:28, 19.29s/it]\n",
      "{'loss': 2.2668, 'grad_norm': 0.37109375, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "9%|▉         | 30/327 [09:39<1:35:28, 19.29s/it]\n",
      "9%|▉         | 31/327 [09:58<1:35:05, 19.28s/it]\n",
      "10%|▉         | 32/327 [10:17<1:34:44, 19.27s/it]\n",
      "10%|█         | 33/327 [10:37<1:34:32, 19.29s/it]\n",
      "10%|█         | 34/327 [10:56<1:34:09, 19.28s/it]\n",
      "11%|█         | 35/327 [11:15<1:33:47, 19.27s/it]\n",
      "11%|█         | 36/327 [11:35<1:33:37, 19.31s/it]\n",
      "11%|█▏        | 37/327 [11:54<1:33:13, 19.29s/it]\n",
      "12%|█▏        | 38/327 [12:13<1:32:51, 19.28s/it]\n",
      "12%|█▏        | 39/327 [12:32<1:32:37, 19.30s/it]\n",
      "12%|█▏        | 40/327 [12:52<1:32:14, 19.28s/it]\n",
      "{'loss': 2.2196, 'grad_norm': 0.423828125, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "12%|█▏        | 40/327 [12:52<1:32:14, 19.28s/it]\n",
      "13%|█▎        | 41/327 [13:11<1:31:52, 19.27s/it]\n",
      "13%|█▎        | 42/327 [13:30<1:31:31, 19.27s/it]\n",
      "13%|█▎        | 43/327 [13:50<1:31:18, 19.29s/it]\n",
      "13%|█▎        | 44/327 [14:09<1:30:56, 19.28s/it]\n",
      "14%|█▍        | 45/327 [14:28<1:30:34, 19.27s/it]\n",
      "14%|█▍        | 46/327 [14:47<1:30:25, 19.31s/it]\n",
      "14%|█▍        | 47/327 [15:07<1:30:01, 19.29s/it]\n",
      "15%|█▍        | 48/327 [15:26<1:29:39, 19.28s/it]\n",
      "15%|█▍        | 49/327 [15:45<1:29:25, 19.30s/it]\n",
      "15%|█▌        | 50/327 [16:05<1:29:02, 19.29s/it]\n",
      "{'loss': 2.1645, 'grad_norm': 0.5078125, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "15%|█▌        | 50/327 [16:05<1:29:02, 19.29s/it]\n",
      "16%|█▌        | 51/327 [16:24<1:28:40, 19.28s/it]\n",
      "16%|█▌        | 52/327 [16:43<1:28:19, 19.27s/it]\n",
      "16%|█▌        | 53/327 [17:02<1:28:06, 19.29s/it]\n",
      "17%|█▋        | 54/327 [17:22<1:27:44, 19.28s/it]\n",
      "17%|█▋        | 55/327 [17:41<1:27:22, 19.27s/it]\n",
      "17%|█▋        | 56/327 [18:00<1:27:12, 19.31s/it]\n",
      "17%|█▋        | 57/327 [18:20<1:26:48, 19.29s/it]\n",
      "18%|█▊        | 58/327 [18:39<1:26:26, 19.28s/it]\n",
      "18%|█▊        | 59/327 [18:58<1:26:12, 19.30s/it]\n",
      "18%|█▊        | 60/327 [19:17<1:25:49, 19.29s/it]\n",
      "{'loss': 2.0649, 'grad_norm': 0.419921875, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "18%|█▊        | 60/327 [19:17<1:25:49, 19.29s/it]\n",
      "19%|█▊        | 61/327 [19:37<1:25:27, 19.28s/it]\n",
      "19%|█▉        | 62/327 [19:56<1:25:16, 19.31s/it]\n",
      "19%|█▉        | 63/327 [20:15<1:24:53, 19.29s/it]\n",
      "20%|█▉        | 64/327 [20:35<1:24:30, 19.28s/it]\n",
      "20%|█▉        | 65/327 [20:54<1:24:09, 19.27s/it]\n",
      "20%|██        | 66/327 [21:13<1:23:56, 19.30s/it]\n",
      "20%|██        | 67/327 [21:32<1:23:33, 19.28s/it]\n",
      "21%|██        | 68/327 [21:52<1:23:12, 19.28s/it]\n",
      "21%|██        | 69/327 [22:11<1:22:59, 19.30s/it]\n",
      "21%|██▏       | 70/327 [22:30<1:22:36, 19.29s/it]\n",
      "{'loss': 2.1958, 'grad_norm': 0.447265625, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "21%|██▏       | 70/327 [22:30<1:22:36, 19.29s/it]\n",
      "22%|██▏       | 71/327 [22:50<1:22:15, 19.28s/it]\n",
      "22%|██▏       | 72/327 [23:09<1:22:04, 19.31s/it]\n",
      "22%|██▏       | 73/327 [23:28<1:21:40, 19.29s/it]\n",
      "23%|██▎       | 74/327 [23:47<1:21:18, 19.28s/it]\n",
      "23%|██▎       | 75/327 [24:07<1:20:57, 19.27s/it]\n",
      "23%|██▎       | 76/327 [24:26<1:20:43, 19.30s/it]\n",
      "24%|██▎       | 77/327 [24:45<1:20:21, 19.28s/it]\n",
      "24%|██▍       | 78/327 [25:05<1:19:59, 19.28s/it]\n",
      "24%|██▍       | 79/327 [25:24<1:19:46, 19.30s/it]\n",
      "24%|██▍       | 80/327 [25:43<1:19:23, 19.29s/it]\n",
      "{'loss': 2.0754, 'grad_norm': 0.470703125, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "24%|██▍       | 80/327 [25:43<1:19:23, 19.29s/it]\n",
      "25%|██▍       | 81/327 [26:02<1:19:02, 19.28s/it]\n",
      "25%|██▌       | 82/327 [26:22<1:18:51, 19.31s/it]\n",
      "25%|██▌       | 83/327 [26:41<1:18:28, 19.30s/it]\n",
      "26%|██▌       | 84/327 [27:00<1:18:05, 19.28s/it]\n",
      "26%|██▌       | 85/327 [27:20<1:17:44, 19.28s/it]\n",
      "26%|██▋       | 86/327 [27:39<1:17:30, 19.30s/it]\n",
      "27%|██▋       | 87/327 [27:58<1:17:08, 19.28s/it]\n",
      "27%|██▋       | 88/327 [28:17<1:16:47, 19.28s/it]\n",
      "27%|██▋       | 89/327 [28:37<1:16:33, 19.30s/it]\n",
      "28%|██▊       | 90/327 [28:56<1:16:11, 19.29s/it]\n",
      "{'loss': 1.9514, 'grad_norm': 0.416015625, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "28%|██▊       | 90/327 [28:56<1:16:11, 19.29s/it]\n",
      "28%|██▊       | 91/327 [29:15<1:15:49, 19.28s/it]\n",
      "28%|██▊       | 92/327 [29:35<1:15:37, 19.31s/it]\n",
      "28%|██▊       | 93/327 [29:54<1:15:14, 19.29s/it]\n",
      "29%|██▊       | 94/327 [30:13<1:14:52, 19.28s/it]\n",
      "29%|██▉       | 95/327 [30:32<1:14:31, 19.27s/it]\n",
      "29%|██▉       | 96/327 [30:52<1:14:17, 19.30s/it]\n",
      "30%|██▉       | 97/327 [31:11<1:13:55, 19.29s/it]\n",
      "30%|██▉       | 98/327 [31:30<1:13:34, 19.28s/it]\n",
      "30%|███       | 99/327 [31:50<1:13:20, 19.30s/it]\n",
      "31%|███       | 100/327 [32:09<1:12:58, 19.29s/it]\n",
      "{'loss': 2.1185, 'grad_norm': 0.39453125, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "31%|███       | 100/327 [32:09<1:12:58, 19.29s/it]\n",
      "31%|███       | 101/327 [32:28<1:12:37, 19.28s/it]\n",
      "31%|███       | 102/327 [32:48<1:12:24, 19.31s/it]\n",
      "31%|███▏      | 103/327 [33:07<1:12:01, 19.29s/it]\n",
      "32%|███▏      | 104/327 [33:26<1:11:39, 19.28s/it]\n",
      "32%|███▏      | 105/327 [33:45<1:11:18, 19.27s/it]\n",
      "32%|███▏      | 106/327 [34:05<1:11:04, 19.29s/it]\n",
      "33%|███▎      | 107/327 [34:24<1:10:42, 19.28s/it]\n",
      "33%|███▎      | 108/327 [34:43<1:10:20, 19.27s/it]\n",
      "33%|███▎      | 109/327 [35:03<1:10:06, 19.30s/it]\n",
      "34%|███▎      | 110/327 [35:22<1:10:15, 19.42s/it]\n",
      "{'loss': 1.9977, 'grad_norm': 0.4296875, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
      "34%|███▎      | 110/327 [35:22<1:10:15, 19.42s/it]\n",
      "34%|███▍      | 111/327 [35:42<1:09:44, 19.37s/it]\n",
      "34%|███▍      | 112/327 [36:01<1:09:24, 19.37s/it]\n",
      "35%|███▍      | 113/327 [36:20<1:08:58, 19.34s/it]\n",
      "35%|███▍      | 114/327 [36:39<1:08:33, 19.31s/it]\n",
      "35%|███▌      | 115/327 [36:59<1:08:16, 19.32s/it]\n",
      "35%|███▌      | 116/327 [37:18<1:07:53, 19.31s/it]\n",
      "36%|███▌      | 117/327 [37:37<1:07:31, 19.29s/it]\n",
      "36%|███▌      | 118/327 [37:57<1:07:09, 19.28s/it]\n",
      "36%|███▋      | 119/327 [38:16<1:06:55, 19.30s/it]\n",
      "37%|███▋      | 120/327 [38:35<1:06:32, 19.29s/it]\n",
      "{'loss': 1.7827, 'grad_norm': 0.4765625, 'learning_rate': 0.0002, 'epoch': 1.1}\n",
      "37%|███▋      | 120/327 [38:35<1:06:32, 19.29s/it]\n",
      "37%|███▋      | 121/327 [38:54<1:06:11, 19.28s/it]\n",
      "37%|███▋      | 122/327 [39:14<1:05:56, 19.30s/it]\n",
      "38%|███▊      | 123/327 [39:33<1:05:34, 19.29s/it]\n",
      "38%|███▊      | 124/327 [39:52<1:05:13, 19.28s/it]\n",
      "38%|███▊      | 125/327 [40:12<1:04:59, 19.30s/it]\n",
      "39%|███▊      | 126/327 [40:31<1:04:37, 19.29s/it]\n",
      "39%|███▉      | 127/327 [40:50<1:04:15, 19.28s/it]\n",
      "39%|███▉      | 128/327 [41:09<1:04:00, 19.30s/it]\n",
      "39%|███▉      | 129/327 [41:29<1:03:38, 19.29s/it]\n",
      "40%|███▉      | 130/327 [41:48<1:03:17, 19.28s/it]\n",
      "{'loss': 1.9144, 'grad_norm': 0.5078125, 'learning_rate': 0.0002, 'epoch': 1.19}\n",
      "40%|███▉      | 130/327 [41:48<1:03:17, 19.28s/it]\n",
      "40%|████      | 131/327 [42:07<1:02:56, 19.27s/it]\n",
      "40%|████      | 132/327 [42:27<1:02:42, 19.29s/it]\n",
      "41%|████      | 133/327 [42:46<1:02:20, 19.28s/it]\n",
      "41%|████      | 134/327 [43:05<1:01:59, 19.27s/it]\n",
      "41%|████▏     | 135/327 [43:24<1:01:46, 19.31s/it]\n",
      "42%|████▏     | 136/327 [43:44<1:01:24, 19.29s/it]\n",
      "42%|████▏     | 137/327 [44:03<1:01:03, 19.28s/it]\n",
      "42%|████▏     | 138/327 [44:22<1:00:47, 19.30s/it]\n",
      "43%|████▎     | 139/327 [44:42<1:00:25, 19.29s/it]\n",
      "43%|████▎     | 140/327 [45:01<1:00:04, 19.28s/it]\n",
      "{'loss': 1.8968, 'grad_norm': 0.7734375, 'learning_rate': 0.0002, 'epoch': 1.28}\n",
      "43%|████▎     | 140/327 [45:01<1:00:04, 19.28s/it]\n",
      "43%|████▎     | 141/327 [45:20<59:44, 19.27s/it]\n",
      "43%|████▎     | 142/327 [45:39<59:29, 19.29s/it]\n",
      "44%|████▎     | 143/327 [45:59<59:07, 19.28s/it]\n",
      "44%|████▍     | 144/327 [46:18<58:47, 19.27s/it]\n",
      "44%|████▍     | 145/327 [46:37<58:33, 19.31s/it]\n",
      "45%|████▍     | 146/327 [46:57<58:11, 19.29s/it]\n",
      "45%|████▍     | 147/327 [47:16<57:50, 19.28s/it]\n",
      "45%|████▌     | 148/327 [47:35<57:34, 19.30s/it]\n",
      "46%|████▌     | 149/327 [47:54<57:12, 19.29s/it]\n",
      "46%|████▌     | 150/327 [48:14<56:51, 19.28s/it]\n",
      "{'loss': 1.8515, 'grad_norm': 0.46875, 'learning_rate': 0.0002, 'epoch': 1.38}\n",
      "46%|████▌     | 150/327 [48:14<56:51, 19.28s/it]\n",
      "46%|████▌     | 151/327 [48:33<56:31, 19.27s/it]\n",
      "46%|████▋     | 152/327 [48:52<56:16, 19.29s/it]\n",
      "47%|████▋     | 153/327 [49:12<55:55, 19.28s/it]\n",
      "47%|████▋     | 154/327 [49:31<55:34, 19.27s/it]\n",
      "47%|████▋     | 155/327 [49:50<55:20, 19.31s/it]\n",
      "48%|████▊     | 156/327 [50:09<54:58, 19.29s/it]\n",
      "48%|████▊     | 157/327 [50:29<54:37, 19.28s/it]\n",
      "48%|████▊     | 158/327 [50:48<54:21, 19.30s/it]\n",
      "49%|████▊     | 159/327 [51:07<53:59, 19.28s/it]\n",
      "49%|████▉     | 160/327 [51:27<53:38, 19.28s/it]\n",
      "{'loss': 1.8851, 'grad_norm': 0.67578125, 'learning_rate': 0.0002, 'epoch': 1.47}\n",
      "49%|████▉     | 160/327 [51:27<53:38, 19.28s/it]\n",
      "49%|████▉     | 161/327 [51:46<53:18, 19.27s/it]\n",
      "50%|████▉     | 162/327 [52:05<53:03, 19.29s/it]\n",
      "50%|████▉     | 163/327 [52:24<52:42, 19.28s/it]\n",
      "50%|█████     | 164/327 [52:44<52:21, 19.27s/it]\n",
      "50%|█████     | 165/327 [53:03<52:07, 19.30s/it]\n",
      "51%|█████     | 166/327 [53:22<51:45, 19.29s/it]\n",
      "51%|█████     | 167/327 [53:42<51:24, 19.28s/it]\n",
      "51%|█████▏    | 168/327 [54:01<51:08, 19.30s/it]\n",
      "52%|█████▏    | 169/327 [54:20<50:47, 19.29s/it]\n",
      "52%|█████▏    | 170/327 [54:39<50:28, 19.29s/it]\n",
      "{'loss': 1.8593, 'grad_norm': 0.5546875, 'learning_rate': 0.0002, 'epoch': 1.56}\n",
      "52%|█████▏    | 170/327 [54:39<50:28, 19.29s/it]\n",
      "52%|█████▏    | 171/327 [54:59<50:07, 19.28s/it]\n",
      "53%|█████▎    | 172/327 [55:18<49:51, 19.30s/it]\n",
      "53%|█████▎    | 173/327 [55:37<49:29, 19.29s/it]\n",
      "53%|█████▎    | 174/327 [55:57<49:09, 19.28s/it]\n",
      "54%|█████▎    | 175/327 [56:16<48:54, 19.31s/it]\n",
      "54%|█████▍    | 176/327 [56:35<48:32, 19.29s/it]\n",
      "54%|█████▍    | 177/327 [56:54<48:11, 19.28s/it]\n",
      "54%|█████▍    | 178/327 [57:14<47:55, 19.30s/it]\n",
      "55%|█████▍    | 179/327 [57:33<47:34, 19.28s/it]\n",
      "55%|█████▌    | 180/327 [57:52<47:13, 19.27s/it]\n",
      "{'loss': 1.8146, 'grad_norm': 0.494140625, 'learning_rate': 0.0002, 'epoch': 1.65}\n",
      "55%|█████▌    | 180/327 [57:52<47:13, 19.27s/it]\n",
      "55%|█████▌    | 181/327 [58:12<46:53, 19.27s/it]\n",
      "56%|█████▌    | 182/327 [58:31<46:37, 19.29s/it]\n",
      "56%|█████▌    | 183/327 [58:50<46:16, 19.28s/it]\n",
      "56%|█████▋    | 184/327 [59:09<45:55, 19.27s/it]\n",
      "57%|█████▋    | 185/327 [59:29<45:41, 19.31s/it]\n",
      "57%|█████▋    | 186/327 [59:48<45:19, 19.29s/it]\n",
      "57%|█████▋    | 187/327 [1:00:07<44:58, 19.28s/it]\n",
      "57%|█████▋    | 188/327 [1:00:27<44:42, 19.30s/it]\n",
      "58%|█████▊    | 189/327 [1:00:46<44:21, 19.29s/it]\n",
      "58%|█████▊    | 190/327 [1:01:05<44:00, 19.28s/it]\n",
      "{'loss': 1.6878, 'grad_norm': 0.61328125, 'learning_rate': 0.0002, 'epoch': 1.74}\n",
      "58%|█████▊    | 190/327 [1:01:05<44:00, 19.28s/it]\n",
      "58%|█████▊    | 191/327 [1:01:24<43:40, 19.27s/it]\n",
      "59%|█████▊    | 192/327 [1:01:44<43:24, 19.29s/it]\n",
      "59%|█████▉    | 193/327 [1:02:03<43:03, 19.28s/it]\n",
      "59%|█████▉    | 194/327 [1:02:22<42:43, 19.27s/it]\n",
      "60%|█████▉    | 195/327 [1:02:42<42:28, 19.31s/it]\n",
      "60%|█████▉    | 196/327 [1:03:01<42:06, 19.29s/it]\n",
      "60%|██████    | 197/327 [1:03:20<41:46, 19.28s/it]\n",
      "61%|██████    | 198/327 [1:03:39<41:29, 19.30s/it]\n",
      "61%|██████    | 199/327 [1:03:59<41:08, 19.28s/it]\n",
      "61%|██████    | 200/327 [1:04:18<40:48, 19.28s/it]\n",
      "{'loss': 1.7984, 'grad_norm': 0.52734375, 'learning_rate': 0.0002, 'epoch': 1.83}\n",
      "61%|██████    | 200/327 [1:04:18<40:48, 19.28s/it]\n",
      "61%|██████▏   | 201/327 [1:04:37<40:27, 19.27s/it]\n",
      "62%|██████▏   | 202/327 [1:04:57<40:11, 19.29s/it]\n",
      "62%|██████▏   | 203/327 [1:05:16<39:50, 19.28s/it]\n",
      "62%|██████▏   | 204/327 [1:05:35<39:30, 19.27s/it]\n",
      "63%|██████▎   | 205/327 [1:05:54<39:15, 19.30s/it]\n",
      "63%|██████▎   | 206/327 [1:06:14<38:53, 19.29s/it]\n",
      "63%|██████▎   | 207/327 [1:06:33<38:33, 19.28s/it]\n",
      "64%|██████▎   | 208/327 [1:06:52<38:16, 19.30s/it]\n",
      "64%|██████▍   | 209/327 [1:07:12<37:55, 19.29s/it]\n",
      "64%|██████▍   | 210/327 [1:07:31<37:35, 19.28s/it]\n",
      "{'loss': 1.7032, 'grad_norm': 0.484375, 'learning_rate': 0.0002, 'epoch': 1.93}\n",
      "64%|██████▍   | 210/327 [1:07:31<37:35, 19.28s/it]\n",
      "65%|██████▍   | 211/327 [1:07:50<37:15, 19.27s/it]\n",
      "65%|██████▍   | 212/327 [1:08:09<36:58, 19.29s/it]\n",
      "65%|██████▌   | 213/327 [1:08:29<36:38, 19.28s/it]\n",
      "65%|██████▌   | 214/327 [1:08:48<36:17, 19.27s/it]\n",
      "66%|██████▌   | 215/327 [1:09:07<36:02, 19.31s/it]\n",
      "66%|██████▌   | 216/327 [1:09:27<35:41, 19.29s/it]\n",
      "66%|██████▋   | 217/327 [1:09:46<35:20, 19.28s/it]\n",
      "67%|██████▋   | 218/327 [1:10:05<35:03, 19.30s/it]\n",
      "67%|██████▋   | 219/327 [1:10:25<34:57, 19.42s/it]\n",
      "67%|██████▋   | 220/327 [1:10:44<34:32, 19.37s/it]\n",
      "{'loss': 1.743, 'grad_norm': 0.59375, 'learning_rate': 0.0002, 'epoch': 2.02}\n",
      "67%|██████▋   | 220/327 [1:10:44<34:32, 19.37s/it]\n",
      "68%|██████▊   | 221/327 [1:11:04<34:13, 19.38s/it]\n",
      "68%|██████▊   | 222/327 [1:11:23<33:50, 19.34s/it]\n",
      "68%|██████▊   | 223/327 [1:11:42<33:28, 19.31s/it]\n",
      "69%|██████▊   | 224/327 [1:12:01<33:07, 19.30s/it]\n",
      "69%|██████▉   | 225/327 [1:12:21<32:49, 19.31s/it]\n",
      "69%|██████▉   | 226/327 [1:12:40<32:28, 19.30s/it]\n",
      "69%|██████▉   | 227/327 [1:12:59<32:08, 19.28s/it]\n",
      "70%|██████▉   | 228/327 [1:13:19<31:51, 19.31s/it]\n",
      "70%|███████   | 229/327 [1:13:38<31:30, 19.29s/it]\n",
      "70%|███████   | 230/327 [1:13:57<31:10, 19.28s/it]\n",
      "{'loss': 1.4983, 'grad_norm': 0.6171875, 'learning_rate': 0.0002, 'epoch': 2.11}\n",
      "70%|███████   | 230/327 [1:13:57<31:10, 19.28s/it]\n",
      "71%|███████   | 231/327 [1:14:16<30:53, 19.31s/it]\n",
      "71%|███████   | 232/327 [1:14:36<30:32, 19.29s/it]\n",
      "71%|███████▏  | 233/327 [1:14:55<30:12, 19.28s/it]\n",
      "72%|███████▏  | 234/327 [1:15:14<29:52, 19.28s/it]\n",
      "72%|███████▏  | 235/327 [1:15:34<29:35, 19.30s/it]\n",
      "72%|███████▏  | 236/327 [1:15:53<29:14, 19.28s/it]\n",
      "72%|███████▏  | 237/327 [1:16:12<28:54, 19.28s/it]\n",
      "73%|███████▎  | 238/327 [1:16:31<28:37, 19.30s/it]\n",
      "73%|███████▎  | 239/327 [1:16:51<28:17, 19.29s/it]\n",
      "73%|███████▎  | 240/327 [1:17:10<27:57, 19.28s/it]\n",
      "{'loss': 1.6147, 'grad_norm': 0.5234375, 'learning_rate': 0.0002, 'epoch': 2.2}\n",
      "73%|███████▎  | 240/327 [1:17:10<27:57, 19.28s/it]\n",
      "74%|███████▎  | 241/327 [1:17:29<27:40, 19.31s/it]\n",
      "74%|███████▍  | 242/327 [1:17:49<27:19, 19.29s/it]\n",
      "74%|███████▍  | 243/327 [1:18:08<26:59, 19.28s/it]\n",
      "75%|███████▍  | 244/327 [1:18:27<26:39, 19.27s/it]\n",
      "75%|███████▍  | 245/327 [1:18:46<26:22, 19.30s/it]\n",
      "75%|███████▌  | 246/327 [1:19:06<26:01, 19.28s/it]\n",
      "76%|███████▌  | 247/327 [1:19:25<25:42, 19.28s/it]\n",
      "76%|███████▌  | 248/327 [1:19:44<25:24, 19.30s/it]\n",
      "76%|███████▌  | 249/327 [1:20:04<25:04, 19.29s/it]\n",
      "76%|███████▋  | 250/327 [1:20:23<24:44, 19.28s/it]\n",
      "{'loss': 1.5089, 'grad_norm': 0.66015625, 'learning_rate': 0.0002, 'epoch': 2.29}\n",
      "76%|███████▋  | 250/327 [1:20:23<24:44, 19.28s/it]\n",
      "77%|███████▋  | 251/327 [1:20:42<24:27, 19.31s/it]\n",
      "77%|███████▋  | 252/327 [1:21:01<24:06, 19.29s/it]\n",
      "77%|███████▋  | 253/327 [1:21:21<23:46, 19.28s/it]\n",
      "78%|███████▊  | 254/327 [1:21:40<23:26, 19.27s/it]\n",
      "78%|███████▊  | 255/327 [1:21:59<23:09, 19.30s/it]\n",
      "78%|███████▊  | 256/327 [1:22:19<22:49, 19.28s/it]\n",
      "79%|███████▊  | 257/327 [1:22:38<22:29, 19.28s/it]\n",
      "79%|███████▉  | 258/327 [1:22:57<22:11, 19.30s/it]\n",
      "79%|███████▉  | 259/327 [1:23:16<21:51, 19.29s/it]\n",
      "80%|███████▉  | 260/327 [1:23:36<21:31, 19.28s/it]\n",
      "{'loss': 1.5999, 'grad_norm': 0.703125, 'learning_rate': 0.0002, 'epoch': 2.39}\n",
      "80%|███████▉  | 260/327 [1:23:36<21:31, 19.28s/it]\n",
      "80%|███████▉  | 261/327 [1:23:55<21:14, 19.31s/it]\n",
      "80%|████████  | 262/327 [1:24:14<20:54, 19.29s/it]\n",
      "80%|████████  | 263/327 [1:24:34<20:33, 19.28s/it]\n",
      "81%|████████  | 264/327 [1:24:53<20:14, 19.27s/it]\n",
      "81%|████████  | 265/327 [1:25:12<19:56, 19.30s/it]\n",
      "81%|████████▏ | 266/327 [1:25:31<19:36, 19.28s/it]\n",
      "82%|████████▏ | 267/327 [1:25:51<19:16, 19.27s/it]\n",
      "82%|████████▏ | 268/327 [1:26:10<18:58, 19.30s/it]\n",
      "82%|████████▏ | 269/327 [1:26:29<18:38, 19.29s/it]\n",
      "83%|████████▎ | 270/327 [1:26:49<18:18, 19.28s/it]\n",
      "{'loss': 1.5398, 'grad_norm': 0.71875, 'learning_rate': 0.0002, 'epoch': 2.48}\n",
      "83%|████████▎ | 270/327 [1:26:49<18:18, 19.28s/it]\n",
      "83%|████████▎ | 271/327 [1:27:08<18:01, 19.32s/it]\n",
      "83%|████████▎ | 272/327 [1:27:27<17:41, 19.30s/it]\n",
      "83%|████████▎ | 273/327 [1:27:46<17:21, 19.29s/it]\n",
      "84%|████████▍ | 274/327 [1:28:06<17:01, 19.28s/it]\n",
      "84%|████████▍ | 275/327 [1:28:25<16:43, 19.31s/it]\n",
      "84%|████████▍ | 276/327 [1:28:44<16:23, 19.29s/it]\n",
      "85%|████████▍ | 277/327 [1:29:04<16:04, 19.28s/it]\n",
      "85%|████████▌ | 278/327 [1:29:23<15:45, 19.30s/it]\n",
      "85%|████████▌ | 279/327 [1:29:42<15:25, 19.29s/it]\n",
      "86%|████████▌ | 280/327 [1:30:01<15:06, 19.28s/it]\n",
      "{'loss': 1.4983, 'grad_norm': 0.59375, 'learning_rate': 0.0002, 'epoch': 2.57}\n",
      "86%|████████▌ | 280/327 [1:30:01<15:06, 19.28s/it]\n",
      "86%|████████▌ | 281/327 [1:30:21<14:48, 19.31s/it]\n",
      "86%|████████▌ | 282/327 [1:30:40<14:28, 19.30s/it]\n",
      "87%|████████▋ | 283/327 [1:30:59<14:08, 19.28s/it]\n",
      "87%|████████▋ | 284/327 [1:31:19<13:48, 19.27s/it]\n",
      "87%|████████▋ | 285/327 [1:31:38<13:30, 19.30s/it]\n",
      "87%|████████▋ | 286/327 [1:31:57<13:10, 19.28s/it]\n",
      "88%|████████▊ | 287/327 [1:32:16<12:51, 19.28s/it]\n",
      "88%|████████▊ | 288/327 [1:32:36<12:32, 19.30s/it]\n",
      "88%|████████▊ | 289/327 [1:32:55<12:12, 19.28s/it]\n",
      "89%|████████▊ | 290/327 [1:33:14<11:53, 19.28s/it]\n",
      "{'loss': 1.4023, 'grad_norm': 0.56640625, 'learning_rate': 0.0002, 'epoch': 2.66}\n",
      "89%|████████▊ | 290/327 [1:33:14<11:53, 19.28s/it]\n",
      "89%|████████▉ | 291/327 [1:33:34<11:34, 19.30s/it]\n",
      "89%|████████▉ | 292/327 [1:33:53<11:15, 19.29s/it]\n",
      "90%|████████▉ | 293/327 [1:34:12<10:55, 19.28s/it]\n",
      "90%|████████▉ | 294/327 [1:34:31<10:36, 19.27s/it]\n",
      "90%|█████████ | 295/327 [1:34:51<10:17, 19.31s/it]\n",
      "91%|█████████ | 296/327 [1:35:10<09:58, 19.29s/it]\n",
      "91%|█████████ | 297/327 [1:35:29<09:38, 19.28s/it]\n",
      "91%|█████████ | 298/327 [1:35:49<09:19, 19.30s/it]\n",
      "91%|█████████▏| 299/327 [1:36:08<09:00, 19.29s/it]\n",
      "92%|█████████▏| 300/327 [1:36:27<08:40, 19.28s/it]\n",
      "{'loss': 1.468, 'grad_norm': 0.66015625, 'learning_rate': 0.0002, 'epoch': 2.75}\n",
      "92%|█████████▏| 300/327 [1:36:27<08:40, 19.28s/it]\n",
      "92%|█████████▏| 301/327 [1:36:47<08:21, 19.31s/it]\n",
      "92%|█████████▏| 302/327 [1:37:06<08:02, 19.29s/it]\n",
      "93%|█████████▎| 303/327 [1:37:25<07:42, 19.28s/it]\n",
      "93%|█████████▎| 304/327 [1:37:44<07:23, 19.27s/it]\n",
      "93%|█████████▎| 305/327 [1:38:04<07:04, 19.31s/it]\n",
      "94%|█████████▎| 306/327 [1:38:23<06:45, 19.29s/it]\n",
      "94%|█████████▍| 307/327 [1:38:42<06:25, 19.28s/it]\n",
      "94%|█████████▍| 308/327 [1:39:02<06:06, 19.30s/it]\n",
      "94%|█████████▍| 309/327 [1:39:21<05:47, 19.29s/it]\n",
      "95%|█████████▍| 310/327 [1:39:40<05:27, 19.28s/it]\n",
      "{'loss': 1.4504, 'grad_norm': 0.78125, 'learning_rate': 0.0002, 'epoch': 2.84}\n",
      "95%|█████████▍| 310/327 [1:39:40<05:27, 19.28s/it]\n",
      "95%|█████████▌| 311/327 [1:39:59<05:08, 19.31s/it]\n",
      "95%|█████████▌| 312/327 [1:40:19<04:49, 19.29s/it]\n",
      "96%|█████████▌| 313/327 [1:40:38<04:29, 19.28s/it]\n",
      "96%|█████████▌| 314/327 [1:40:57<04:10, 19.27s/it]\n",
      "96%|█████████▋| 315/327 [1:41:17<03:51, 19.31s/it]\n",
      "97%|█████████▋| 316/327 [1:41:36<03:32, 19.29s/it]\n",
      "97%|█████████▋| 317/327 [1:41:55<03:12, 19.28s/it]\n",
      "97%|█████████▋| 318/327 [1:42:15<02:53, 19.30s/it]\n",
      "98%|█████████▊| 319/327 [1:42:34<02:34, 19.29s/it]\n",
      "98%|█████████▊| 320/327 [1:42:53<02:14, 19.28s/it]\n",
      "{'loss': 1.4639, 'grad_norm': 0.7421875, 'learning_rate': 0.0002, 'epoch': 2.94}\n",
      "98%|█████████▊| 320/327 [1:42:53<02:14, 19.28s/it]\n",
      "98%|█████████▊| 321/327 [1:43:12<01:55, 19.30s/it]\n",
      "98%|█████████▊| 322/327 [1:43:32<01:36, 19.29s/it]\n",
      "99%|█████████▉| 323/327 [1:43:51<01:17, 19.28s/it]\n",
      "99%|█████████▉| 324/327 [1:44:10<00:57, 19.27s/it]\n",
      "99%|█████████▉| 325/327 [1:44:30<00:38, 19.30s/it]\n",
      "100%|█████████▉| 326/327 [1:44:49<00:19, 19.29s/it]\n",
      "100%|██████████| 327/327 [1:45:08<00:00, 19.28s/it]\n",
      "{'train_runtime': 6318.2816, 'train_samples_per_second': 0.207, 'train_steps_per_second': 0.052, 'train_loss': 1.8314262559289962, 'epoch': 3.0}\n",
      "100%|██████████| 327/327 [1:45:09<00:00, 19.28s/it]\n",
      "100%|██████████| 327/327 [1:45:09<00:00, 19.29s/it]\n",
      "['checkpoint-109', 'tokenizer.json', 'adapter_model.safetensors', 'special_tokens_map.json', 'README.md', 'tokenizer_config.json', 'checkpoint-327', 'adapter_config.json', 'checkpoint-218', 'tokenizer.model']\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.45s/it]\n",
      "wandb: - 0.003 MB of 0.040 MB uploaded\n",
      "wandb: \\ 0.040 MB of 0.040 MB uploaded\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "wandb:              train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "wandb:                train/grad_norm ▁▂▂▃▄▃▃▄▃▂▃▄▄█▄▆▅▄▆▄▄▅▆▄▆▇▇▅▅▆█▇\n",
      "wandb:            train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:                     train/loss █▇▇▆▆▅▆▅▅▆▅▃▄▄▄▄▄▄▃▄▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "wandb:               train/total_flos ▁\n",
      "wandb:               train/train_loss ▁\n",
      "wandb:            train/train_runtime ▁\n",
      "wandb: train/train_samples_per_second ▁\n",
      "wandb:   train/train_steps_per_second ▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                    train/epoch 3.0\n",
      "wandb:              train/global_step 327\n",
      "wandb:                train/grad_norm 0.74219\n",
      "wandb:            train/learning_rate 0.0002\n",
      "wandb:                     train/loss 1.4639\n",
      "wandb:               train/total_flos 1.724425095515996e+17\n",
      "wandb:               train/train_loss 1.83143\n",
      "wandb:            train/train_runtime 6318.2816\n",
      "wandb: train/train_samples_per_second 0.207\n",
      "wandb:   train/train_steps_per_second 0.052\n",
      "wandb:\n",
      "wandb: 🚀 View run mistralaiMistral7BInstructv01 at: https://wandb.ai/danielpradilla/mistral-tuning-sagemaker/runs/mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111-pzfyaf-algo-1\n",
      "wandb: ⭐️ View project at: https://wandb.ai/danielpradilla/mistral-tuning-sagemaker\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20240424_155039-mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111-pzfyaf-algo-1/logs\n",
      "2024-04-24 17:37:59,737 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-04-24 17:37:59,737 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-04-24 17:37:59,737 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2024-04-24 17:38:06 Uploading - Uploading generated training model\n",
      "2024-04-24 17:39:11 Completed - Training job completed\n",
      "Training seconds: 6849\n",
      "Billable seconds: 6849\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': f\"{s3_training_folder_uri}\" ,'validation': f\"{s3_evaluation_folder_uri}\"}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get trained model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-510357808667/mistralaiMistral7BInstructv01-2024-04-24-15-43-52-111/output/model/'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "We are going to use the Hugging Face LLM Inference DLC a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by Text Generation Inference (TGI) solution for deploying and serving Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2024-04-24-20-48-14-158\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-04-24-20-48-15-276\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2024-04-24-20-48-15-276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "\n",
    "Original training data was diary entries for an author at a certain age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Escribe una entrada media de diario por ROBELLO CHEBELO MAMELO cuando tenía 36 años[/INST]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json \n",
    "import re\n",
    "# Read all lines from the file\n",
    "with open(LOCAL_EVAL_FILE, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Select one random line\n",
    "random_line = random.choice(lines)\n",
    "\n",
    "# Optionally, you might want to convert the JSON line into a Python dictionary\n",
    "import json\n",
    "data = json.loads(random_line)\n",
    "\n",
    "# Use regex to find content between [INST] and [/INST]\n",
    "match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', data['text'])\n",
    "if match:\n",
    "    inst_content = match.group(0)\n",
    "else:\n",
    "    inst_content = \"No match found\"\n",
    "\n",
    "print(inst_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Escribe una entrada media de diario por ROBELLO CHEBELO MAMELO cuando tenía 36 años[/INST]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ayer en la noche, mi línea de fuego de árboles se trocó y el tráfico se detuvo durante un largo rato en la Carretera de la Playa, pelo en medio de la carretera y dos autos subiendo nuevamente el camino de tierra que desemboca en la autopista. La gente creó brevemente una árgilla, todos los que pudimos, quisimos, tuvimos que seguir caminando.\\n\\nDupá valió la pena, a pesar de que en realidad Dupá no es tan especial, nada de lo que pasa en Dupá puede suceder en Caracas; a pesar de que ninguna señora puede acordar ir ahí y disfrazarse de una víbora en los próximos 50 años. Dupá está muy lejos, prehistórica, un eco de cuando Chacao era el cuarto de Caracas (en vez de el cuarto menos importante, un cuarto que sólo se mantuvo quizás por su proximidad a Dupá). Dupá tiene el desaforo natural que da la distancia, el caos organizado del pasado.\\n\\nQue lástima. De lo que pasa en Dupá yo a pesar de mi egoísmo paternal, después de todo río que me ha oxidado, fisicamente y espiritualmente, quiero decir: no puedo ni quisiera acordarme de lo que pasó en Dupá, porque después de todo lo que a mí me parece una estúpida humillación, una desilusión eterna y sin final (quisiera decir: eterna y sin final o que estás culpable de tu propia humillación,\") y hemos cambiado tan poco en 15 años, nada que ahora, después de 15 años, cada vez que veo una foto de mi en el pasado me parezca inexplicablemientos feliz, como la paz entre la pareja o un acuerdo entre la gente del país. Nada. Veinticinco años de asesinos gays, de puerquitos, de odio, de nada.'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "def request(prompt):\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "        \"temperature\": 0.9\n",
    "      }\n",
    "    })\n",
    "    return outputs[0]['generated_text']\n",
    "\n",
    "print(inst_content)\n",
    "request(inst_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/150 requests\n",
      "Processed 2/150 requests\n",
      "Processed 3/150 requests\n",
      "Processed 4/150 requests\n",
      "Processed 5/150 requests\n",
      "Processed 6/150 requests\n",
      "Processed 7/150 requests\n",
      "Processed 8/150 requests\n",
      "Processed 9/150 requests\n",
      "Processed 10/150 requests\n",
      "Processed 11/150 requests\n",
      "Processed 12/150 requests\n",
      "Processed 13/150 requests\n",
      "Processed 14/150 requests\n",
      "Processed 15/150 requests\n",
      "Processed 16/150 requests\n",
      "Processed 17/150 requests\n",
      "Processed 18/150 requests\n",
      "Processed 19/150 requests\n",
      "Processed 20/150 requests\n",
      "Processed 21/150 requests\n",
      "Processed 22/150 requests\n",
      "Processed 23/150 requests\n",
      "Processed 24/150 requests\n",
      "Processed 25/150 requests\n",
      "Processed 26/150 requests\n",
      "Processed 27/150 requests\n",
      "Processed 28/150 requests\n",
      "Processed 29/150 requests\n",
      "Processed 30/150 requests\n",
      "Processed 31/150 requests\n",
      "Processed 32/150 requests\n",
      "Processed 33/150 requests\n",
      "Processed 34/150 requests\n",
      "Processed 35/150 requests\n",
      "Processed 36/150 requests\n",
      "Processed 37/150 requests\n",
      "Processed 38/150 requests\n",
      "Processed 39/150 requests\n",
      "Processed 40/150 requests\n",
      "Processed 41/150 requests\n",
      "Processed 42/150 requests\n",
      "Processed 43/150 requests\n",
      "Processed 44/150 requests\n",
      "Processed 45/150 requests\n",
      "Processed 46/150 requests\n",
      "Processed 47/150 requests\n",
      "Processed 48/150 requests\n",
      "Processed 49/150 requests\n",
      "Processed 50/150 requests\n",
      "Processed 51/150 requests\n",
      "Processed 52/150 requests\n",
      "Processed 53/150 requests\n",
      "Processed 54/150 requests\n",
      "Processed 55/150 requests\n",
      "Processed 56/150 requests\n",
      "Processed 57/150 requests\n",
      "Processed 58/150 requests\n",
      "Processed 59/150 requests\n",
      "Processed 60/150 requests\n",
      "Processed 61/150 requests\n",
      "Processed 62/150 requests\n",
      "Processed 63/150 requests\n",
      "Processed 64/150 requests\n",
      "Processed 65/150 requests\n",
      "Processed 66/150 requests\n",
      "Processed 67/150 requests\n",
      "Processed 68/150 requests\n",
      "Processed 69/150 requests\n",
      "Processed 70/150 requests\n",
      "Processed 71/150 requests\n",
      "Processed 72/150 requests\n",
      "Processed 73/150 requests\n",
      "Processed 74/150 requests\n",
      "Processed 75/150 requests\n",
      "Processed 76/150 requests\n",
      "Processed 77/150 requests\n",
      "Processed 78/150 requests\n",
      "Processed 79/150 requests\n",
      "Processed 80/150 requests\n",
      "Processed 81/150 requests\n",
      "Processed 82/150 requests\n",
      "Processed 83/150 requests\n",
      "Processed 84/150 requests\n",
      "Processed 85/150 requests\n",
      "Processed 86/150 requests\n",
      "Processed 87/150 requests\n",
      "Processed 88/150 requests\n",
      "Processed 89/150 requests\n",
      "Processed 90/150 requests\n",
      "Processed 91/150 requests\n",
      "Processed 92/150 requests\n",
      "Processed 93/150 requests\n",
      "Processed 94/150 requests\n",
      "Processed 95/150 requests\n",
      "Processed 96/150 requests\n",
      "Processed 97/150 requests\n",
      "Processed 98/150 requests\n",
      "Processed 99/150 requests\n",
      "Processed 100/150 requests\n",
      "Processed 101/150 requests\n",
      "Processed 102/150 requests\n",
      "Processed 103/150 requests\n",
      "Processed 104/150 requests\n",
      "Processed 105/150 requests\n",
      "Processed 106/150 requests\n",
      "Processed 107/150 requests\n",
      "Processed 108/150 requests\n",
      "Processed 109/150 requests\n",
      "Processed 110/150 requests\n",
      "Processed 111/150 requests\n",
      "Processed 112/150 requests\n",
      "Processed 113/150 requests\n",
      "Processed 114/150 requests\n",
      "Processed 115/150 requests\n",
      "Processed 116/150 requests\n",
      "Processed 117/150 requests\n",
      "Processed 118/150 requests\n",
      "Processed 119/150 requests\n",
      "Processed 120/150 requests\n",
      "Processed 121/150 requests\n",
      "Processed 122/150 requests\n",
      "Processed 123/150 requests\n",
      "Processed 124/150 requests\n",
      "Processed 125/150 requests\n",
      "Processed 126/150 requests\n",
      "Processed 127/150 requests\n",
      "Processed 128/150 requests\n",
      "Processed 129/150 requests\n",
      "Processed 130/150 requests\n",
      "Processed 131/150 requests\n",
      "Processed 132/150 requests\n",
      "Processed 133/150 requests\n",
      "Processed 134/150 requests\n",
      "Processed 135/150 requests\n",
      "Processed 136/150 requests\n",
      "Processed 137/150 requests\n",
      "Processed 138/150 requests\n",
      "Processed 139/150 requests\n",
      "Processed 140/150 requests\n",
      "Processed 141/150 requests\n",
      "Processed 142/150 requests\n",
      "Processed 143/150 requests\n",
      "Processed 144/150 requests\n",
      "Processed 145/150 requests\n",
      "Processed 146/150 requests\n",
      "Processed 147/150 requests\n",
      "Processed 148/150 requests\n",
      "Processed 149/150 requests\n",
      "Processed 150/150 requests\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define the basic prompt template\n",
    "base_prompt = \"[INST]Escribe una entrada {length} de diario por ROBELLO CHEBELO MAMELO cuando tenía {age} años[/INST]\"\n",
    "\n",
    "# Define possible lengths and age range\n",
    "lengths = ['corta', 'media', 'larga']\n",
    "ages = range(19, 46) \n",
    "\n",
    "# Generate records\n",
    "records = []\n",
    "for _ in range(150):\n",
    "    length_choice = random.choice(lengths)\n",
    "    age_choice = random.choice(ages)\n",
    "    prompt = base_prompt.format(length=length_choice, age=age_choice)\n",
    "    records.append({'prompt': prompt})\n",
    "\n",
    "total = len(records)\n",
    "for index, item in enumerate(records):\n",
    "    response = request(item['prompt'])  # Call the request function\n",
    "    item['response'] = response\n",
    "    print(f\"Processed {index + 1}/{total} requests\")  # Progress indicator\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('data/generated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2024-04-24-20-48-14-158\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2024-04-24-20-48-15-276\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2024-04-24-20-48-15-276\n"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
