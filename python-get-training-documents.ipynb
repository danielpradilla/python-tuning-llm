{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import chardet\n",
    "import textract\n",
    "import pyarrow\n",
    "import random\n",
    "from dotenv import load_dotenv, find_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv(), override=True) # read local .env file\n",
    "TEXT_FILES_PATH = os.getenv('TEXT_FILES_PATH')  # Get the value of the API_KEY variable\n",
    "BIRTH_DATE = os.getenv('BIRTH_DATE')\n",
    "AUTHOR_NAME = os.getenv('AUTHOR_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            while True:\n",
    "                chunk = file.read(1024)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                # Check for null bytes in the chunk\n",
    "                if b'\\x00' in chunk:\n",
    "                    return True\n",
    "                \n",
    "                # Check for non-printable characters in the chunk\n",
    "                if any(byte < 32 and byte not in (9, 10, 13) for byte in chunk):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking the file: {e}\")\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to store the data\n",
    "data=[]\n",
    "# Define the comparison date\n",
    "comparison_date = datetime.strptime(BIRTH_DATE, '%Y-%m-%d')\n",
    "\n",
    "# Walk through the directory containing text files\n",
    "\n",
    "for root, dirs, files in os.walk(TEXT_FILES_PATH):\n",
    "    for file in files:\n",
    "        if file=='.DS_Store':\n",
    "            continue\n",
    "        # Get the date and year of the file\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_stat = os.stat(file_path)\n",
    "        file_timestamp = datetime.fromtimestamp(file_stat.st_mtime)\n",
    "        file_date = datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d')\n",
    "        file_year = datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y')\n",
    "\n",
    "        # Check if the filename follows the DDMMYY format\n",
    "        if len(file) == 10 and file[:6].isdigit():\n",
    "            try:\n",
    "                # Extract the date from the filename\n",
    "                file_day = int(file[:2])\n",
    "                file_month = int(file[2:4])\n",
    "                file_year = 1900 + int(file[4:6])\n",
    "                \n",
    "                # Create a datetime object from the extracted date\n",
    "                file_date = datetime(file_year, file_month, file_day)\n",
    "                file_timestamp = datetime.fromtimestamp(file_date.timestamp())\n",
    "                file_date = file_date.strftime('%Y-%m-%d')\n",
    "                file_year = file_timestamp.strftime('%Y')\n",
    "\n",
    "            except ValueError:\n",
    "                print(f\"File: {file}, Invalid Date Format\")\n",
    "\n",
    "        # Calculate the difference in years\n",
    "        year_difference = file_timestamp.year - comparison_date.year - ((file_timestamp.month, file_timestamp.day) < (comparison_date.month, comparison_date.day))\n",
    "\n",
    "        # Split the file name to get the extension (if any)\n",
    "        _, file_extension = os.path.splitext(file)\n",
    "        file_extension=file_extension.lower()\n",
    "        if not is_binary_file(file_path) and (file_extension==\".txt\" or not file_extension):\n",
    "            #get the encoding\n",
    "            with open(file_path, 'rb') as f:\n",
    "                result = chardet.detect(f.read())\n",
    "                if (result['encoding']=='Windows-1252' and not file_extension):\n",
    "                    encoding = 'MacRoman'\n",
    "                else:\n",
    "                    encoding = result['encoding']\n",
    "            # Read the content of the file\n",
    "            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                content = f.read()\n",
    "        if file_extension==\".doc\" or file_extension==\".docx\":\n",
    "            #maybe a doc file\n",
    "            if file_extension==\".doc\":\n",
    "                encoding = 'iso-8859-1'\n",
    "            else:\n",
    "                encoding = 'utf-8'\n",
    "            try:\n",
    "                content = textract.process(file_path, input_encoding=encoding).decode('utf-8')\n",
    "\n",
    "#                content = textract.process(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {file_path} {e}\")\n",
    "                    # Append the data to the list\n",
    "\n",
    "        word_count = len(content.split())  \n",
    "        \n",
    "        data.append([file_path, file_date, file_year, year_difference, content, word_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['source', 'date', 'year', 'age', 'text', 'word_count'])\n",
    "print(df.shape)\n",
    "# Sample 20 rows to inspect the contents\n",
    "df.sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Parquet file\n",
    "df.to_parquet('../data/text_files_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a tuning file\n",
    "tuning_entries = []\n",
    "\n",
    "def sanitize_text(text):\n",
    "    rtn = text\n",
    "    clean = re.compile('<.*?>')\n",
    "    rtn = re.sub(clean, '', text)\n",
    "    # Escape double quotes within the JSON\n",
    "    rtn = rtn.replace('\"', '\\\\\"')\n",
    "    return rtn\n",
    "\n",
    "def get_word_count_classification(word_count):\n",
    "    rtn=\"corta\"\n",
    "    if word_count<=100:\n",
    "        rtn=\"corta\"\n",
    "    elif word_count<=500:\n",
    "        rtn=\"media\"\n",
    "    else:\n",
    "        rtn=\"larga\"\n",
    "    return rtn\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    age = row['age']\n",
    "    text = row['text']\n",
    "    wc_class = get_word_count_classification(row['word_count'])\n",
    "    # Apply the function to the 'Content' column\n",
    "    text = sanitize_text(text)\n",
    "    prompt = f'Escribe una entrada {wc_class} de diario por {AUTHOR_NAME} cuando tenía {age} años'\n",
    "    entry = {'prompt': prompt, 'completion': text} #generic instruction format\n",
    "    #entry = {\"text\":f\"<s>[INST]{prompt}[/INST] {text}</s>\"} #mistral expected format\n",
    "    tuning_entries.append(entry)\n",
    "\n",
    "# Save the tuning file to a text file\n",
    "with open('../data/tuning_entries_all.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for entry in tuning_entries:\n",
    "        file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(\"Tuning file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2143 samples for training and 535 samples for validation.\n"
     ]
    }
   ],
   "source": [
    "# Define the ratio of data to use for validation (e.g., 20% for validation)\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Calculate the number of validation samples based on the ratio\n",
    "num_validation_samples = int(len(tuning_entries) * validation_ratio)\n",
    "\n",
    "# Randomly shuffle the data\n",
    "random.shuffle(tuning_entries)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "training_data = tuning_entries[num_validation_samples:]\n",
    "validation_data = tuning_entries[:num_validation_samples]\n",
    "\n",
    "# Save the training and validation datasets to separate files\n",
    "with open('../data/tuning_training_entries.jsonl', 'w', encoding='utf-8') as train_file:\n",
    "    for item in training_data:\n",
    "        train_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('../data/tuning_validation_entries.jsonl', 'w', encoding='utf-8') as valid_file:\n",
    "    for item in validation_data:\n",
    "        valid_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"Split {len(training_data)} samples for training and {len(validation_data)} samples for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
